{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974cc0c4-35e7-4feb-9eb6-a1053bc57574",
   "metadata": {},
   "source": [
    "## Model Ensembling: Stacking and Blending - Part 04\n",
    "\n",
    "Ensembling is a powerful technique in machine learning where multiple models (often referred to as \"base models\" or \"weak learners\") are combined to produce a stronger model with improved predictive performance. \n",
    "\n",
    "**Two popular ensembling techniques are Stacking and Blending.**\n",
    "\n",
    "**1. Stacking (Stacked Generalization)**\n",
    "\n",
    "+ Stacking involves training multiple different models and then training a \"meta-model\" (also called a \"stacker\") on their outputs.\n",
    "+ The idea is that the meta-model learns how to best combine the predictions from the base models to achieve the highest accuracy.\n",
    "\n",
    "**How Stacking Works**\n",
    "+ Base Models: Train several different models on the same dataset. These models can be of different types, such as a Logistic Regression, Decision Tree, Random Forest, etc.\n",
    "+ Meta-Model: A meta-model (e.g., another Logistic Regression or a Gradient Boosting model) is trained on the predictions of the base models. The meta-model learns to weigh the base models' predictions to minimize the overall error.\n",
    "\n",
    "**Explanation:**\n",
    "+ Base Models: Random Forest, Gradient Boosting, and SVM are used as base models in this example.\n",
    "+ Meta-Model: A Logistic Regression model serves as the meta-model that combines the predictions of the base models.\n",
    "+ Cross-Validation (cv=5): The Stacking Classifier uses cross-validation to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660366e9-7108-4b73-87da-46908b88ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b9cd683-c7b6-4bd3-9e5d-5ea04a8a7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Accuracy: 0.8055358410220014\n",
      "Classification Report for Stacking Model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      1036\n",
      "           1       0.68      0.51      0.58       373\n",
      "\n",
      "    accuracy                           0.81      1409\n",
      "   macro avg       0.76      0.71      0.73      1409\n",
      "weighted avg       0.79      0.81      0.80      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## stacking algorithms\n",
    "# Load your data\n",
    "data = pd.read_csv('processed_customer_data.csv')\n",
    "features = data.drop(['Churn', 'customerID'], axis=1)\n",
    "target = data['Churn']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Stacking Model Accuracy:\", accuracy)\n",
    "print(\"Classification Report for Stacking Model:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc5d04-bc52-4aaa-9f50-f6f918f7de72",
   "metadata": {},
   "source": [
    "**2. Blending**\n",
    "\n",
    "+ Blending is similar to stacking but slightly simpler.\n",
    "+ It typically involves creating a holdout set (a small subset of the training data) to train the meta-model, rather than using cross-validation.\n",
    "\n",
    "**How Blending Works:**\n",
    "+ Split Data: Split the training data into two parts (e.g., 80% and 20%).\n",
    "+ Train Base Models: Train the base models on the larger part (e.g., 80%).\n",
    "+ Generate Predictions: Make predictions on the smaller holdout set (e.g., 20%).\n",
    "+ Meta-Model Training: Train a meta-model on the predictions from the base models on the holdout set.\n",
    "+ Final Prediction: Use the meta-model to predict on the test set.\n",
    "\n",
    "**Explanation**\n",
    "+ Base Models: Random Forest, Gradient Boosting, and SVM.\n",
    "+ Holdout Set: Used to create predictions for the meta-model.\n",
    "+ Meta-Model: A Logistic Regression model that combines the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ead14cf-0c15-416f-86eb-e180e512fdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blended Model Accuracy: 0.8105039034776437\n",
      "Classification Report for Blended Model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      1036\n",
      "           1       0.69      0.51      0.59       373\n",
      "\n",
      "    accuracy                           0.81      1409\n",
      "   macro avg       0.77      0.72      0.73      1409\n",
      "weighted avg       0.80      0.81      0.80      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv('processed_customer_data.csv')\n",
    "features = data.drop(['Churn', 'customerID'], axis=1)\n",
    "target = data['Churn']\n",
    "\n",
    "# Split the data into training and holdout sets (blending step)\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train base models on the main training set\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the holdout set\n",
    "rf_pred = rf.predict_proba(X_holdout)[:, 1]\n",
    "gb_pred = gb.predict_proba(X_holdout)[:, 1]\n",
    "svc_pred = svc.predict_proba(X_holdout)[:, 1]\n",
    "\n",
    "# Create new features based on the predictions\n",
    "X_meta = np.column_stack((rf_pred, gb_pred, svc_pred))\n",
    "\n",
    "# Train the meta-model on these new features\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(X_meta, y_holdout)\n",
    "\n",
    "# Make final predictions on the test set\n",
    "rf_pred_test = rf.predict_proba(X_test)[:, 1]\n",
    "gb_pred_test = gb.predict_proba(X_test)[:, 1]\n",
    "svc_pred_test = svc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "X_meta_test = np.column_stack((rf_pred_test, gb_pred_test, svc_pred_test))\n",
    "y_pred_final = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluate the blended model\n",
    "accuracy = accuracy_score(y_test, y_pred_final)\n",
    "report = classification_report(y_test, y_pred_final)\n",
    "print(\"Blended Model Accuracy:\", accuracy)\n",
    "print(\"Classification Report for Blended Model:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ebb16-4b58-4c77-82cf-f5cec251fab3",
   "metadata": {},
   "source": [
    "**Key Differences Between Stacking and Blending:**\n",
    "+ Stacking uses cross-validation to train the meta-model, while Blending uses a holdout set.\n",
    "+ Blending is simpler to implement but might not utilize all the training data as effectively as Stacking.\n",
    "\n",
    "\n",
    "**Which One to Choose?**\n",
    "+ Stacking is usually more robust but can be computationally more intensive.\n",
    "+ Blending is simpler and quicker but may not perform as well if the holdout set is not representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f078fe-92de-42ea-90db-a3e7445296d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
