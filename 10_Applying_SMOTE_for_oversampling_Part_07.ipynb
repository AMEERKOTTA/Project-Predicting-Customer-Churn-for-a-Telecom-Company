{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457273f7-a602-49b8-8dca-5cdd43df5be8",
   "metadata": {},
   "source": [
    "## Applying SMOTE for Oversampling Part 07\n",
    "\n",
    "**Address Class Imbalance:**\n",
    "\n",
    "+ Since the dataset is imbalanced, apply techniques specifically designed to handle this. Here are some approaches:\n",
    "\n",
    "+ `Oversampling the Minority Class`: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or simple oversampling to create synthetic examples for the minority class.\n",
    "\n",
    "+ `Undersampling the Majority Class`: Reduce the number of samples from the majority class to balance the dataset. However, this could lead to losing important data.\n",
    "\n",
    "+ `Use Class Weights`: Adjust the weights for the classes in your models so that the model pays more attention to the minority class. Many algorithms, including neural networks and TabNet, allow you to set class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ac2199-c9dc-4d22-acbc-5946b007170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffc1cd16-3c99-4fe5-ad9c-95e282b357a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('no_missing_values_customer_data.csv')\n",
    "\n",
    "# Convert the target variable 'Churn' to numeric\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Encode categorical variables using Label Encoding\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    if col != 'customerID':\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "# Separate features and target\n",
    "features = data.drop(['Churn', 'customerID'], axis=1).values\n",
    "target = data['Churn'].values\n",
    "\n",
    "# Apply SMOTE for oversampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "features_resampled, target_resampled = smote.fit_resample(features, target)\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2b5193-1b35-49ca-af07-a69e15c6466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.59131 | val_0_accuracy: 0.64831 |  0:00:03s\n",
      "epoch 1  | loss: 0.51501 | val_0_accuracy: 0.73961 |  0:00:05s\n",
      "epoch 2  | loss: 0.50643 | val_0_accuracy: 0.74831 |  0:00:08s\n",
      "epoch 3  | loss: 0.4963  | val_0_accuracy: 0.74541 |  0:00:10s\n",
      "epoch 4  | loss: 0.48998 | val_0_accuracy: 0.76232 |  0:00:13s\n",
      "epoch 5  | loss: 0.48046 | val_0_accuracy: 0.76957 |  0:00:16s\n",
      "epoch 6  | loss: 0.47429 | val_0_accuracy: 0.77391 |  0:00:19s\n",
      "epoch 7  | loss: 0.47016 | val_0_accuracy: 0.76425 |  0:00:22s\n",
      "epoch 8  | loss: 0.46079 | val_0_accuracy: 0.78019 |  0:00:24s\n",
      "epoch 9  | loss: 0.47276 | val_0_accuracy: 0.78261 |  0:00:27s\n",
      "epoch 10 | loss: 0.47308 | val_0_accuracy: 0.77101 |  0:00:30s\n",
      "epoch 11 | loss: 0.46351 | val_0_accuracy: 0.78261 |  0:00:32s\n",
      "epoch 12 | loss: 0.45261 | val_0_accuracy: 0.77488 |  0:00:35s\n",
      "epoch 13 | loss: 0.4538  | val_0_accuracy: 0.79034 |  0:00:38s\n",
      "epoch 14 | loss: 0.44513 | val_0_accuracy: 0.79517 |  0:00:41s\n",
      "epoch 15 | loss: 0.44658 | val_0_accuracy: 0.78116 |  0:00:44s\n",
      "epoch 16 | loss: 0.44471 | val_0_accuracy: 0.79517 |  0:00:47s\n",
      "epoch 17 | loss: 0.43875 | val_0_accuracy: 0.80338 |  0:00:50s\n",
      "epoch 18 | loss: 0.44571 | val_0_accuracy: 0.80048 |  0:00:53s\n",
      "epoch 19 | loss: 0.43944 | val_0_accuracy: 0.80435 |  0:00:56s\n",
      "epoch 20 | loss: 0.43601 | val_0_accuracy: 0.80435 |  0:00:59s\n",
      "epoch 21 | loss: 0.43542 | val_0_accuracy: 0.80338 |  0:01:02s\n",
      "epoch 22 | loss: 0.4327  | val_0_accuracy: 0.80773 |  0:01:04s\n",
      "epoch 23 | loss: 0.43412 | val_0_accuracy: 0.80097 |  0:01:07s\n",
      "epoch 24 | loss: 0.42804 | val_0_accuracy: 0.79807 |  0:01:10s\n",
      "epoch 25 | loss: 0.42779 | val_0_accuracy: 0.79614 |  0:01:12s\n",
      "epoch 26 | loss: 0.43168 | val_0_accuracy: 0.78889 |  0:01:15s\n",
      "epoch 27 | loss: 0.43557 | val_0_accuracy: 0.79469 |  0:01:18s\n",
      "epoch 28 | loss: 0.42522 | val_0_accuracy: 0.78647 |  0:01:21s\n",
      "epoch 29 | loss: 0.42401 | val_0_accuracy: 0.80531 |  0:01:23s\n",
      "epoch 30 | loss: 0.42879 | val_0_accuracy: 0.80338 |  0:01:26s\n",
      "epoch 31 | loss: 0.42586 | val_0_accuracy: 0.80048 |  0:01:29s\n",
      "epoch 32 | loss: 0.43688 | val_0_accuracy: 0.80097 |  0:01:32s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_accuracy = 0.80773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for TabNet with SMOTE:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80      1021\n",
      "           1       0.80      0.83      0.81      1049\n",
      "\n",
      "    accuracy                           0.81      2070\n",
      "   macro avg       0.81      0.81      0.81      2070\n",
      "weighted avg       0.81      0.81      0.81      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize TabNetClassifier\n",
    "tabnet_clf = TabNetClassifier()\n",
    "\n",
    "# Train TabNet model\n",
    "tabnet_clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tabnet_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report for TabNet with SMOTE:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a11766-e40f-4b8f-9da9-6a03e740c867",
   "metadata": {},
   "source": [
    "## Imporving the SMOTE Oversmapled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8d655-2798-47e2-bf3e-727786fa5a52",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning Using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0776a964-7303-4f84-8d65-496b760c2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be0b43de-9b72-4b02-b7f4-0e3b64db0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('no_missing_values_customer_data.csv')\n",
    "\n",
    "# Convert the target variable 'Churn' to numeric\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Encode categorical variables using Label Encoding\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    if col != 'customerID':\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "# Separate features and target\n",
    "features = data.drop(['Churn', 'customerID'], axis=1).values\n",
    "target = data['Churn'].values\n",
    "\n",
    "# Apply SMOTE for oversampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "features_resampled, target_resampled = smote.fit_resample(features, target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "124e52b5-7d05-45bc-81e3-0775cde606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-07 18:45:01,389] A new study created in memory with name: no-name-7a22dd48-7f19-4ad0-a730-f346c03cd823\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.8011  | val_0_accuracy: 0.64444 |  0:00:02s\n",
      "epoch 1  | loss: 0.6376  | val_0_accuracy: 0.67923 |  0:00:03s\n",
      "epoch 2  | loss: 0.59369 | val_0_accuracy: 0.69324 |  0:00:05s\n",
      "epoch 3  | loss: 0.58041 | val_0_accuracy: 0.72754 |  0:00:08s\n",
      "epoch 4  | loss: 0.56052 | val_0_accuracy: 0.72512 |  0:00:11s\n",
      "epoch 5  | loss: 0.54879 | val_0_accuracy: 0.72995 |  0:00:14s\n",
      "epoch 6  | loss: 0.54015 | val_0_accuracy: 0.75024 |  0:00:17s\n",
      "epoch 7  | loss: 0.52615 | val_0_accuracy: 0.757   |  0:00:19s\n",
      "epoch 8  | loss: 0.5218  | val_0_accuracy: 0.75797 |  0:00:22s\n",
      "epoch 9  | loss: 0.52369 | val_0_accuracy: 0.77391 |  0:00:25s\n",
      "epoch 10 | loss: 0.51652 | val_0_accuracy: 0.77343 |  0:00:27s\n",
      "epoch 11 | loss: 0.51081 | val_0_accuracy: 0.77536 |  0:00:30s\n",
      "epoch 12 | loss: 0.49798 | val_0_accuracy: 0.76908 |  0:00:32s\n",
      "epoch 13 | loss: 0.50179 | val_0_accuracy: 0.77585 |  0:00:34s\n",
      "epoch 14 | loss: 0.5069  | val_0_accuracy: 0.76522 |  0:00:36s\n",
      "epoch 15 | loss: 0.52686 | val_0_accuracy: 0.76908 |  0:00:39s\n",
      "epoch 16 | loss: 0.50301 | val_0_accuracy: 0.77536 |  0:00:42s\n",
      "epoch 17 | loss: 0.49775 | val_0_accuracy: 0.78019 |  0:00:44s\n",
      "epoch 18 | loss: 0.50246 | val_0_accuracy: 0.79034 |  0:00:46s\n",
      "epoch 19 | loss: 0.49975 | val_0_accuracy: 0.7657  |  0:00:48s\n",
      "epoch 20 | loss: 0.49571 | val_0_accuracy: 0.77633 |  0:00:50s\n",
      "epoch 21 | loss: 0.49555 | val_0_accuracy: 0.77681 |  0:00:53s\n",
      "epoch 22 | loss: 0.48994 | val_0_accuracy: 0.77246 |  0:00:55s\n",
      "epoch 23 | loss: 0.48183 | val_0_accuracy: 0.77971 |  0:00:57s\n",
      "epoch 24 | loss: 0.48594 | val_0_accuracy: 0.77826 |  0:00:59s\n",
      "epoch 25 | loss: 0.4802  | val_0_accuracy: 0.76812 |  0:01:01s\n",
      "epoch 26 | loss: 0.4847  | val_0_accuracy: 0.78309 |  0:01:04s\n",
      "epoch 27 | loss: 0.48462 | val_0_accuracy: 0.78261 |  0:01:07s\n",
      "epoch 28 | loss: 0.48323 | val_0_accuracy: 0.78792 |  0:01:09s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_accuracy = 0.79034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 18:46:18,414] Trial 0 finished with value: 0.7903381642512077 and parameters: {'n_d': 38, 'n_a': 12, 'n_steps': 5, 'gamma': 1.851012759830538, 'lambda_sparse': 0.0015634995647997355, 'lr': 0.0021002571125329043, 'mask_type': 'sparsemax', 'n_shared': 1, 'n_independent': 2}. Best is trial 0 with value: 0.7903381642512077.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.95303 | val_0_accuracy: 0.63285 |  0:00:05s\n",
      "epoch 1  | loss: 0.70689 | val_0_accuracy: 0.67536 |  0:00:11s\n",
      "epoch 2  | loss: 0.63966 | val_0_accuracy: 0.70097 |  0:00:17s\n",
      "epoch 3  | loss: 0.63358 | val_0_accuracy: 0.73237 |  0:00:22s\n",
      "epoch 4  | loss: 0.59308 | val_0_accuracy: 0.7401  |  0:00:30s\n",
      "epoch 5  | loss: 0.57875 | val_0_accuracy: 0.74203 |  0:00:38s\n",
      "epoch 6  | loss: 0.61578 | val_0_accuracy: 0.76329 |  0:00:45s\n",
      "epoch 7  | loss: 0.56759 | val_0_accuracy: 0.74348 |  0:00:50s\n",
      "epoch 8  | loss: 0.55283 | val_0_accuracy: 0.74686 |  0:00:54s\n",
      "epoch 9  | loss: 0.54542 | val_0_accuracy: 0.76039 |  0:00:59s\n",
      "epoch 10 | loss: 0.53859 | val_0_accuracy: 0.73575 |  0:01:03s\n",
      "epoch 11 | loss: 0.55914 | val_0_accuracy: 0.75411 |  0:01:07s\n",
      "epoch 12 | loss: 0.53917 | val_0_accuracy: 0.74879 |  0:01:11s\n",
      "epoch 13 | loss: 0.53441 | val_0_accuracy: 0.76087 |  0:01:15s\n",
      "epoch 14 | loss: 0.53002 | val_0_accuracy: 0.76618 |  0:01:20s\n",
      "epoch 15 | loss: 0.56033 | val_0_accuracy: 0.75797 |  0:01:25s\n",
      "epoch 16 | loss: 0.52766 | val_0_accuracy: 0.75507 |  0:01:30s\n",
      "epoch 17 | loss: 0.52327 | val_0_accuracy: 0.76957 |  0:01:34s\n",
      "epoch 18 | loss: 0.52059 | val_0_accuracy: 0.76715 |  0:01:39s\n",
      "epoch 19 | loss: 0.52889 | val_0_accuracy: 0.7715  |  0:01:44s\n",
      "epoch 20 | loss: 0.50643 | val_0_accuracy: 0.77488 |  0:01:49s\n",
      "epoch 21 | loss: 0.50593 | val_0_accuracy: 0.77198 |  0:01:54s\n",
      "epoch 22 | loss: 0.51137 | val_0_accuracy: 0.78116 |  0:02:00s\n",
      "epoch 23 | loss: 0.50314 | val_0_accuracy: 0.77246 |  0:02:05s\n",
      "epoch 24 | loss: 0.50218 | val_0_accuracy: 0.77295 |  0:02:10s\n",
      "epoch 25 | loss: 0.50468 | val_0_accuracy: 0.77246 |  0:02:15s\n",
      "epoch 26 | loss: 0.49408 | val_0_accuracy: 0.78116 |  0:02:21s\n",
      "epoch 27 | loss: 0.49249 | val_0_accuracy: 0.78841 |  0:02:26s\n",
      "epoch 28 | loss: 0.49695 | val_0_accuracy: 0.78357 |  0:02:31s\n",
      "epoch 29 | loss: 0.48126 | val_0_accuracy: 0.78406 |  0:02:36s\n",
      "epoch 30 | loss: 0.48299 | val_0_accuracy: 0.78599 |  0:02:42s\n",
      "epoch 31 | loss: 0.48542 | val_0_accuracy: 0.79324 |  0:02:47s\n",
      "epoch 32 | loss: 0.48456 | val_0_accuracy: 0.78068 |  0:02:52s\n",
      "epoch 33 | loss: 0.47852 | val_0_accuracy: 0.78986 |  0:02:58s\n",
      "epoch 34 | loss: 0.48074 | val_0_accuracy: 0.79324 |  0:03:03s\n",
      "epoch 35 | loss: 0.4984  | val_0_accuracy: 0.8     |  0:03:09s\n",
      "epoch 36 | loss: 0.47909 | val_0_accuracy: 0.78454 |  0:03:15s\n",
      "epoch 37 | loss: 0.46049 | val_0_accuracy: 0.78164 |  0:03:20s\n",
      "epoch 38 | loss: 0.46562 | val_0_accuracy: 0.79469 |  0:03:25s\n",
      "epoch 39 | loss: 0.4574  | val_0_accuracy: 0.79275 |  0:03:31s\n",
      "epoch 40 | loss: 0.45625 | val_0_accuracy: 0.78841 |  0:03:38s\n",
      "epoch 41 | loss: 0.46469 | val_0_accuracy: 0.79855 |  0:03:44s\n",
      "epoch 42 | loss: 0.46733 | val_0_accuracy: 0.78647 |  0:03:50s\n",
      "epoch 43 | loss: 0.47129 | val_0_accuracy: 0.7971  |  0:03:55s\n",
      "epoch 44 | loss: 0.46686 | val_0_accuracy: 0.79662 |  0:04:00s\n",
      "epoch 45 | loss: 0.46202 | val_0_accuracy: 0.78744 |  0:04:05s\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_accuracy = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 18:50:26,374] Trial 1 finished with value: 0.8 and parameters: {'n_d': 62, 'n_a': 33, 'n_steps': 7, 'gamma': 1.639451683595382, 'lambda_sparse': 0.0018270698424881831, 'lr': 0.0017941163567835791, 'mask_type': 'sparsemax', 'n_shared': 2, 'n_independent': 3}. Best is trial 1 with value: 0.8.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.23755 | val_0_accuracy: 0.54251 |  0:00:01s\n",
      "epoch 1  | loss: 0.71656 | val_0_accuracy: 0.61787 |  0:00:04s\n",
      "epoch 2  | loss: 0.64609 | val_0_accuracy: 0.65845 |  0:00:06s\n",
      "epoch 3  | loss: 0.60329 | val_0_accuracy: 0.67729 |  0:00:08s\n",
      "epoch 4  | loss: 0.58689 | val_0_accuracy: 0.70097 |  0:00:10s\n",
      "epoch 5  | loss: 0.56999 | val_0_accuracy: 0.72367 |  0:00:11s\n",
      "epoch 6  | loss: 0.55869 | val_0_accuracy: 0.73671 |  0:00:13s\n",
      "epoch 7  | loss: 0.55439 | val_0_accuracy: 0.73527 |  0:00:15s\n",
      "epoch 8  | loss: 0.53998 | val_0_accuracy: 0.75797 |  0:00:17s\n",
      "epoch 9  | loss: 0.53828 | val_0_accuracy: 0.75604 |  0:00:19s\n",
      "epoch 10 | loss: 0.52692 | val_0_accuracy: 0.75845 |  0:00:21s\n",
      "epoch 11 | loss: 0.52444 | val_0_accuracy: 0.75507 |  0:00:23s\n",
      "epoch 12 | loss: 0.51561 | val_0_accuracy: 0.75845 |  0:00:25s\n",
      "epoch 13 | loss: 0.52147 | val_0_accuracy: 0.76329 |  0:00:27s\n",
      "epoch 14 | loss: 0.51283 | val_0_accuracy: 0.76232 |  0:00:29s\n",
      "epoch 15 | loss: 0.50977 | val_0_accuracy: 0.76957 |  0:00:31s\n",
      "epoch 16 | loss: 0.51403 | val_0_accuracy: 0.7657  |  0:00:33s\n",
      "epoch 17 | loss: 0.51426 | val_0_accuracy: 0.76135 |  0:00:35s\n",
      "epoch 18 | loss: 0.50548 | val_0_accuracy: 0.76087 |  0:00:37s\n",
      "epoch 19 | loss: 0.50085 | val_0_accuracy: 0.76957 |  0:00:39s\n",
      "epoch 20 | loss: 0.50422 | val_0_accuracy: 0.76957 |  0:00:41s\n",
      "epoch 21 | loss: 0.49183 | val_0_accuracy: 0.7744  |  0:00:43s\n",
      "epoch 22 | loss: 0.491   | val_0_accuracy: 0.77198 |  0:00:45s\n",
      "epoch 23 | loss: 0.48812 | val_0_accuracy: 0.76908 |  0:00:47s\n",
      "epoch 24 | loss: 0.4902  | val_0_accuracy: 0.7744  |  0:00:49s\n",
      "epoch 25 | loss: 0.48812 | val_0_accuracy: 0.77295 |  0:00:52s\n",
      "epoch 26 | loss: 0.48853 | val_0_accuracy: 0.7686  |  0:00:53s\n",
      "epoch 27 | loss: 0.48403 | val_0_accuracy: 0.78116 |  0:00:55s\n",
      "epoch 28 | loss: 0.48294 | val_0_accuracy: 0.76957 |  0:00:57s\n",
      "epoch 29 | loss: 0.47457 | val_0_accuracy: 0.78068 |  0:00:59s\n",
      "epoch 30 | loss: 0.47529 | val_0_accuracy: 0.77343 |  0:01:01s\n",
      "epoch 31 | loss: 0.47741 | val_0_accuracy: 0.7686  |  0:01:03s\n",
      "epoch 32 | loss: 0.48155 | val_0_accuracy: 0.78213 |  0:01:05s\n",
      "epoch 33 | loss: 0.4732  | val_0_accuracy: 0.78116 |  0:01:07s\n",
      "epoch 34 | loss: 0.46937 | val_0_accuracy: 0.78696 |  0:01:08s\n",
      "epoch 35 | loss: 0.46445 | val_0_accuracy: 0.78744 |  0:01:10s\n",
      "epoch 36 | loss: 0.46128 | val_0_accuracy: 0.7913  |  0:01:12s\n",
      "epoch 37 | loss: 0.45804 | val_0_accuracy: 0.79034 |  0:01:14s\n",
      "epoch 38 | loss: 0.45709 | val_0_accuracy: 0.78357 |  0:01:16s\n",
      "epoch 39 | loss: 0.45315 | val_0_accuracy: 0.79275 |  0:01:19s\n",
      "epoch 40 | loss: 0.4543  | val_0_accuracy: 0.79952 |  0:01:21s\n",
      "epoch 41 | loss: 0.45177 | val_0_accuracy: 0.79227 |  0:01:23s\n",
      "epoch 42 | loss: 0.4456  | val_0_accuracy: 0.79034 |  0:01:24s\n",
      "epoch 43 | loss: 0.44858 | val_0_accuracy: 0.79517 |  0:01:26s\n",
      "epoch 44 | loss: 0.44945 | val_0_accuracy: 0.79614 |  0:01:28s\n",
      "epoch 45 | loss: 0.44261 | val_0_accuracy: 0.79469 |  0:01:30s\n",
      "epoch 46 | loss: 0.44624 | val_0_accuracy: 0.79807 |  0:01:31s\n",
      "epoch 47 | loss: 0.44366 | val_0_accuracy: 0.80048 |  0:01:33s\n",
      "epoch 48 | loss: 0.44885 | val_0_accuracy: 0.79952 |  0:01:35s\n",
      "epoch 49 | loss: 0.44414 | val_0_accuracy: 0.80097 |  0:01:37s\n",
      "epoch 50 | loss: 0.44301 | val_0_accuracy: 0.7942  |  0:01:39s\n",
      "epoch 51 | loss: 0.4484  | val_0_accuracy: 0.7942  |  0:01:40s\n",
      "epoch 52 | loss: 0.44302 | val_0_accuracy: 0.79614 |  0:01:42s\n",
      "epoch 53 | loss: 0.44208 | val_0_accuracy: 0.80386 |  0:01:44s\n",
      "epoch 54 | loss: 0.43822 | val_0_accuracy: 0.80242 |  0:01:46s\n",
      "epoch 55 | loss: 0.44543 | val_0_accuracy: 0.79952 |  0:01:47s\n",
      "epoch 56 | loss: 0.44803 | val_0_accuracy: 0.79855 |  0:01:49s\n",
      "epoch 57 | loss: 0.44463 | val_0_accuracy: 0.80483 |  0:01:53s\n",
      "epoch 58 | loss: 0.43542 | val_0_accuracy: 0.79372 |  0:01:57s\n",
      "epoch 59 | loss: 0.43343 | val_0_accuracy: 0.79227 |  0:02:04s\n",
      "epoch 60 | loss: 0.43076 | val_0_accuracy: 0.79855 |  0:02:09s\n",
      "epoch 61 | loss: 0.43426 | val_0_accuracy: 0.80193 |  0:02:13s\n",
      "epoch 62 | loss: 0.44158 | val_0_accuracy: 0.80386 |  0:02:15s\n",
      "epoch 63 | loss: 0.43666 | val_0_accuracy: 0.79662 |  0:02:18s\n",
      "epoch 64 | loss: 0.43375 | val_0_accuracy: 0.81111 |  0:02:20s\n",
      "epoch 65 | loss: 0.43702 | val_0_accuracy: 0.80628 |  0:02:22s\n",
      "epoch 66 | loss: 0.44057 | val_0_accuracy: 0.80338 |  0:02:24s\n",
      "epoch 67 | loss: 0.44318 | val_0_accuracy: 0.8029  |  0:02:27s\n",
      "epoch 68 | loss: 0.44149 | val_0_accuracy: 0.80193 |  0:02:29s\n",
      "epoch 69 | loss: 0.43794 | val_0_accuracy: 0.80725 |  0:02:31s\n",
      "epoch 70 | loss: 0.42933 | val_0_accuracy: 0.80145 |  0:02:34s\n",
      "epoch 71 | loss: 0.43646 | val_0_accuracy: 0.80483 |  0:02:36s\n",
      "epoch 72 | loss: 0.43602 | val_0_accuracy: 0.80242 |  0:02:38s\n",
      "epoch 73 | loss: 0.4365  | val_0_accuracy: 0.80966 |  0:02:40s\n",
      "epoch 74 | loss: 0.43219 | val_0_accuracy: 0.80821 |  0:02:42s\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 64 and best_val_0_accuracy = 0.81111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 18:53:10,704] Trial 2 finished with value: 0.8111111111111111 and parameters: {'n_d': 25, 'n_a': 9, 'n_steps': 4, 'gamma': 1.9859411393133408, 'lambda_sparse': 0.009945299843254766, 'lr': 0.001936965926206663, 'mask_type': 'sparsemax', 'n_shared': 2, 'n_independent': 2}. Best is trial 2 with value: 0.8111111111111111.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.95383 | val_0_accuracy: 0.54976 |  0:00:06s\n",
      "epoch 1  | loss: 0.65692 | val_0_accuracy: 0.69082 |  0:00:11s\n",
      "epoch 2  | loss: 0.60555 | val_0_accuracy: 0.71063 |  0:00:17s\n",
      "epoch 3  | loss: 0.59434 | val_0_accuracy: 0.7372  |  0:00:24s\n",
      "epoch 4  | loss: 0.58334 | val_0_accuracy: 0.70725 |  0:00:31s\n",
      "epoch 5  | loss: 0.56881 | val_0_accuracy: 0.7628  |  0:00:37s\n",
      "epoch 6  | loss: 0.54559 | val_0_accuracy: 0.76184 |  0:00:43s\n",
      "epoch 7  | loss: 0.54109 | val_0_accuracy: 0.77053 |  0:00:49s\n",
      "epoch 8  | loss: 0.51494 | val_0_accuracy: 0.76763 |  0:00:55s\n",
      "epoch 9  | loss: 0.53352 | val_0_accuracy: 0.77923 |  0:01:01s\n",
      "epoch 10 | loss: 0.51451 | val_0_accuracy: 0.75797 |  0:01:06s\n",
      "epoch 11 | loss: 0.53359 | val_0_accuracy: 0.77633 |  0:01:12s\n",
      "epoch 12 | loss: 0.54065 | val_0_accuracy: 0.77778 |  0:01:18s\n",
      "epoch 13 | loss: 0.52501 | val_0_accuracy: 0.77391 |  0:01:24s\n",
      "epoch 14 | loss: 0.53409 | val_0_accuracy: 0.77488 |  0:01:30s\n",
      "epoch 15 | loss: 0.51166 | val_0_accuracy: 0.74686 |  0:01:38s\n",
      "epoch 16 | loss: 0.49683 | val_0_accuracy: 0.76812 |  0:01:45s\n",
      "epoch 17 | loss: 0.48886 | val_0_accuracy: 0.77488 |  0:01:51s\n",
      "epoch 18 | loss: 0.48745 | val_0_accuracy: 0.77488 |  0:01:58s\n",
      "epoch 19 | loss: 0.48586 | val_0_accuracy: 0.78502 |  0:02:04s\n",
      "epoch 20 | loss: 0.48014 | val_0_accuracy: 0.78792 |  0:02:10s\n",
      "epoch 21 | loss: 0.47893 | val_0_accuracy: 0.77826 |  0:02:17s\n",
      "epoch 22 | loss: 0.47491 | val_0_accuracy: 0.79034 |  0:02:24s\n",
      "epoch 23 | loss: 0.47831 | val_0_accuracy: 0.77971 |  0:02:32s\n",
      "epoch 24 | loss: 0.48511 | val_0_accuracy: 0.78213 |  0:02:39s\n",
      "epoch 25 | loss: 0.47042 | val_0_accuracy: 0.79179 |  0:02:47s\n",
      "epoch 26 | loss: 0.46723 | val_0_accuracy: 0.80338 |  0:02:54s\n",
      "epoch 27 | loss: 0.46923 | val_0_accuracy: 0.79614 |  0:03:02s\n",
      "epoch 28 | loss: 0.46261 | val_0_accuracy: 0.78647 |  0:03:10s\n",
      "epoch 29 | loss: 0.45857 | val_0_accuracy: 0.79179 |  0:03:18s\n",
      "epoch 30 | loss: 0.45551 | val_0_accuracy: 0.8058  |  0:03:26s\n",
      "epoch 31 | loss: 0.46202 | val_0_accuracy: 0.78261 |  0:03:34s\n",
      "epoch 32 | loss: 0.46491 | val_0_accuracy: 0.7971  |  0:03:41s\n",
      "epoch 33 | loss: 0.46239 | val_0_accuracy: 0.80531 |  0:03:47s\n",
      "epoch 34 | loss: 0.45845 | val_0_accuracy: 0.8029  |  0:03:53s\n",
      "epoch 35 | loss: 0.4641  | val_0_accuracy: 0.79662 |  0:04:01s\n",
      "epoch 36 | loss: 0.45518 | val_0_accuracy: 0.81256 |  0:04:09s\n",
      "epoch 37 | loss: 0.44757 | val_0_accuracy: 0.79614 |  0:04:17s\n",
      "epoch 38 | loss: 0.46143 | val_0_accuracy: 0.8     |  0:04:25s\n",
      "epoch 39 | loss: 0.46047 | val_0_accuracy: 0.80386 |  0:04:35s\n",
      "epoch 40 | loss: 0.45743 | val_0_accuracy: 0.80531 |  0:04:45s\n",
      "epoch 41 | loss: 0.46353 | val_0_accuracy: 0.78261 |  0:04:54s\n",
      "epoch 42 | loss: 0.4582  | val_0_accuracy: 0.80725 |  0:05:04s\n",
      "epoch 43 | loss: 0.44735 | val_0_accuracy: 0.79469 |  0:05:13s\n",
      "epoch 44 | loss: 0.45199 | val_0_accuracy: 0.80097 |  0:05:22s\n",
      "epoch 45 | loss: 0.45767 | val_0_accuracy: 0.79614 |  0:05:29s\n",
      "epoch 46 | loss: 0.45642 | val_0_accuracy: 0.79324 |  0:05:36s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_accuracy = 0.81256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 18:58:51,341] Trial 3 finished with value: 0.81256038647343 and parameters: {'n_d': 49, 'n_a': 48, 'n_steps': 8, 'gamma': 1.5774190160053538, 'lambda_sparse': 0.009367256371796542, 'lr': 0.002727103127337467, 'mask_type': 'entmax', 'n_shared': 3, 'n_independent': 2}. Best is trial 3 with value: 0.81256038647343.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.86813 | val_0_accuracy: 0.6715  |  0:00:02s\n",
      "epoch 1  | loss: 0.59741 | val_0_accuracy: 0.71449 |  0:00:05s\n",
      "epoch 2  | loss: 0.55518 | val_0_accuracy: 0.72512 |  0:00:08s\n",
      "epoch 3  | loss: 0.54091 | val_0_accuracy: 0.74058 |  0:00:11s\n",
      "epoch 4  | loss: 0.53224 | val_0_accuracy: 0.73285 |  0:00:14s\n",
      "epoch 5  | loss: 0.52112 | val_0_accuracy: 0.74251 |  0:00:16s\n",
      "epoch 6  | loss: 0.52463 | val_0_accuracy: 0.77101 |  0:00:18s\n",
      "epoch 7  | loss: 0.50773 | val_0_accuracy: 0.77536 |  0:00:21s\n",
      "epoch 8  | loss: 0.50778 | val_0_accuracy: 0.78164 |  0:00:24s\n",
      "epoch 9  | loss: 0.50237 | val_0_accuracy: 0.77778 |  0:00:26s\n",
      "epoch 10 | loss: 0.49519 | val_0_accuracy: 0.79517 |  0:00:28s\n",
      "epoch 11 | loss: 0.49455 | val_0_accuracy: 0.79324 |  0:00:31s\n",
      "epoch 12 | loss: 0.48476 | val_0_accuracy: 0.80531 |  0:00:34s\n",
      "epoch 13 | loss: 0.49259 | val_0_accuracy: 0.79517 |  0:00:36s\n",
      "epoch 14 | loss: 0.47721 | val_0_accuracy: 0.78551 |  0:00:39s\n",
      "epoch 15 | loss: 0.47088 | val_0_accuracy: 0.78164 |  0:00:41s\n",
      "epoch 16 | loss: 0.46862 | val_0_accuracy: 0.79324 |  0:00:44s\n",
      "epoch 17 | loss: 0.48237 | val_0_accuracy: 0.79372 |  0:00:46s\n",
      "epoch 18 | loss: 0.45656 | val_0_accuracy: 0.79758 |  0:00:49s\n",
      "epoch 19 | loss: 0.45903 | val_0_accuracy: 0.78647 |  0:00:51s\n",
      "epoch 20 | loss: 0.45462 | val_0_accuracy: 0.79082 |  0:00:54s\n",
      "epoch 21 | loss: 0.45181 | val_0_accuracy: 0.79517 |  0:00:57s\n",
      "epoch 22 | loss: 0.45928 | val_0_accuracy: 0.79565 |  0:00:59s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_accuracy = 0.80531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 18:59:53,527] Trial 4 finished with value: 0.8053140096618358 and parameters: {'n_d': 40, 'n_a': 27, 'n_steps': 6, 'gamma': 1.6830263040540707, 'lambda_sparse': 0.006238563421097442, 'lr': 0.002759147054723279, 'mask_type': 'entmax', 'n_shared': 1, 'n_independent': 1}. Best is trial 3 with value: 0.81256038647343.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.95036 | val_0_accuracy: 0.68696 |  0:00:03s\n",
      "epoch 1  | loss: 0.56556 | val_0_accuracy: 0.74155 |  0:00:08s\n",
      "epoch 2  | loss: 0.52603 | val_0_accuracy: 0.76812 |  0:00:13s\n",
      "epoch 3  | loss: 0.49962 | val_0_accuracy: 0.76667 |  0:00:19s\n",
      "epoch 4  | loss: 0.48876 | val_0_accuracy: 0.77778 |  0:00:23s\n",
      "epoch 5  | loss: 0.47136 | val_0_accuracy: 0.77874 |  0:00:43s\n",
      "epoch 6  | loss: 0.46894 | val_0_accuracy: 0.78309 |  0:00:48s\n",
      "epoch 7  | loss: 0.45701 | val_0_accuracy: 0.78454 |  0:00:54s\n",
      "epoch 8  | loss: 0.43739 | val_0_accuracy: 0.78986 |  0:00:57s\n",
      "epoch 9  | loss: 0.43746 | val_0_accuracy: 0.79082 |  0:01:00s\n",
      "epoch 10 | loss: 0.42732 | val_0_accuracy: 0.78551 |  0:01:03s\n",
      "epoch 11 | loss: 0.43008 | val_0_accuracy: 0.78841 |  0:01:06s\n",
      "epoch 12 | loss: 0.41952 | val_0_accuracy: 0.79034 |  0:01:10s\n",
      "epoch 13 | loss: 0.42594 | val_0_accuracy: 0.79469 |  0:01:13s\n",
      "epoch 14 | loss: 0.41194 | val_0_accuracy: 0.78647 |  0:01:17s\n",
      "epoch 15 | loss: 0.41338 | val_0_accuracy: 0.79614 |  0:01:20s\n",
      "epoch 16 | loss: 0.40417 | val_0_accuracy: 0.7942  |  0:01:24s\n",
      "epoch 17 | loss: 0.40107 | val_0_accuracy: 0.79855 |  0:01:27s\n",
      "epoch 18 | loss: 0.39376 | val_0_accuracy: 0.8058  |  0:01:31s\n",
      "epoch 19 | loss: 0.39064 | val_0_accuracy: 0.8029  |  0:01:34s\n",
      "epoch 20 | loss: 0.39006 | val_0_accuracy: 0.79565 |  0:01:39s\n",
      "epoch 21 | loss: 0.3817  | val_0_accuracy: 0.78986 |  0:01:43s\n",
      "epoch 22 | loss: 0.38152 | val_0_accuracy: 0.79372 |  0:01:48s\n",
      "epoch 23 | loss: 0.37739 | val_0_accuracy: 0.79807 |  0:01:52s\n",
      "epoch 24 | loss: 0.37051 | val_0_accuracy: 0.7971  |  0:01:56s\n",
      "epoch 25 | loss: 0.37323 | val_0_accuracy: 0.80145 |  0:02:01s\n",
      "epoch 26 | loss: 0.36701 | val_0_accuracy: 0.8087  |  0:02:05s\n",
      "epoch 27 | loss: 0.35631 | val_0_accuracy: 0.80628 |  0:02:10s\n",
      "epoch 28 | loss: 0.36047 | val_0_accuracy: 0.8087  |  0:02:14s\n",
      "epoch 29 | loss: 0.36137 | val_0_accuracy: 0.80145 |  0:02:18s\n",
      "epoch 30 | loss: 0.34591 | val_0_accuracy: 0.80435 |  0:02:22s\n",
      "epoch 31 | loss: 0.35228 | val_0_accuracy: 0.80145 |  0:02:27s\n",
      "epoch 32 | loss: 0.35291 | val_0_accuracy: 0.80531 |  0:02:30s\n",
      "epoch 33 | loss: 0.3437  | val_0_accuracy: 0.80435 |  0:02:34s\n",
      "epoch 34 | loss: 0.33931 | val_0_accuracy: 0.80048 |  0:02:37s\n",
      "epoch 35 | loss: 0.34004 | val_0_accuracy: 0.79952 |  0:02:40s\n",
      "epoch 36 | loss: 0.34342 | val_0_accuracy: 0.8058  |  0:02:44s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_accuracy = 0.8087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 19:02:39,826] Trial 5 finished with value: 0.808695652173913 and parameters: {'n_d': 56, 'n_a': 46, 'n_steps': 4, 'gamma': 1.3863947752490262, 'lambda_sparse': 0.004157997279131891, 'lr': 0.0014447417995726575, 'mask_type': 'entmax', 'n_shared': 1, 'n_independent': 3}. Best is trial 3 with value: 0.81256038647343.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.07013 | val_0_accuracy: 0.50097 |  0:00:03s\n",
      "epoch 1  | loss: 0.83189 | val_0_accuracy: 0.57488 |  0:00:07s\n",
      "epoch 2  | loss: 0.73361 | val_0_accuracy: 0.63188 |  0:00:10s\n",
      "epoch 3  | loss: 0.6924  | val_0_accuracy: 0.6913  |  0:00:13s\n",
      "epoch 4  | loss: 0.64174 | val_0_accuracy: 0.72222 |  0:00:17s\n",
      "epoch 5  | loss: 0.61892 | val_0_accuracy: 0.72995 |  0:00:20s\n",
      "epoch 6  | loss: 0.61087 | val_0_accuracy: 0.72657 |  0:00:24s\n",
      "epoch 7  | loss: 0.58724 | val_0_accuracy: 0.73671 |  0:00:27s\n",
      "epoch 8  | loss: 0.59337 | val_0_accuracy: 0.743   |  0:00:30s\n",
      "epoch 9  | loss: 0.57669 | val_0_accuracy: 0.74541 |  0:00:34s\n",
      "epoch 10 | loss: 0.56564 | val_0_accuracy: 0.74734 |  0:00:37s\n",
      "epoch 11 | loss: 0.56132 | val_0_accuracy: 0.75749 |  0:00:41s\n",
      "epoch 12 | loss: 0.5641  | val_0_accuracy: 0.76618 |  0:00:44s\n",
      "epoch 13 | loss: 0.55292 | val_0_accuracy: 0.76763 |  0:00:47s\n",
      "epoch 14 | loss: 0.54612 | val_0_accuracy: 0.75749 |  0:00:50s\n",
      "epoch 15 | loss: 0.54369 | val_0_accuracy: 0.75845 |  0:00:53s\n",
      "epoch 16 | loss: 0.53304 | val_0_accuracy: 0.75266 |  0:00:56s\n",
      "epoch 17 | loss: 0.53061 | val_0_accuracy: 0.75072 |  0:01:00s\n",
      "epoch 18 | loss: 0.53448 | val_0_accuracy: 0.7599  |  0:01:02s\n",
      "epoch 19 | loss: 0.53139 | val_0_accuracy: 0.75749 |  0:01:05s\n",
      "epoch 20 | loss: 0.52038 | val_0_accuracy: 0.76763 |  0:01:08s\n",
      "epoch 21 | loss: 0.51573 | val_0_accuracy: 0.76184 |  0:01:11s\n",
      "epoch 22 | loss: 0.52102 | val_0_accuracy: 0.75749 |  0:01:14s\n",
      "epoch 23 | loss: 0.52025 | val_0_accuracy: 0.76329 |  0:01:18s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_accuracy = 0.76763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 19:03:59,855] Trial 6 finished with value: 0.7676328502415459 and parameters: {'n_d': 64, 'n_a': 37, 'n_steps': 4, 'gamma': 1.6496414164629933, 'lambda_sparse': 0.005485383045884183, 'lr': 0.0003006260164849362, 'mask_type': 'sparsemax', 'n_shared': 3, 'n_independent': 1}. Best is trial 3 with value: 0.81256038647343.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69076 | val_0_accuracy: 0.67971 |  0:00:01s\n",
      "epoch 1  | loss: 0.56483 | val_0_accuracy: 0.71111 |  0:00:03s\n",
      "epoch 2  | loss: 0.52801 | val_0_accuracy: 0.73865 |  0:00:05s\n",
      "epoch 3  | loss: 0.50787 | val_0_accuracy: 0.757   |  0:00:07s\n",
      "epoch 4  | loss: 0.49639 | val_0_accuracy: 0.75652 |  0:00:09s\n",
      "epoch 5  | loss: 0.48939 | val_0_accuracy: 0.76715 |  0:00:11s\n",
      "epoch 6  | loss: 0.4781  | val_0_accuracy: 0.77488 |  0:00:13s\n",
      "epoch 7  | loss: 0.476   | val_0_accuracy: 0.7744  |  0:00:15s\n",
      "epoch 8  | loss: 0.46647 | val_0_accuracy: 0.78068 |  0:00:17s\n",
      "epoch 9  | loss: 0.45708 | val_0_accuracy: 0.78406 |  0:00:19s\n",
      "epoch 10 | loss: 0.45271 | val_0_accuracy: 0.77585 |  0:00:20s\n",
      "epoch 11 | loss: 0.44681 | val_0_accuracy: 0.78889 |  0:00:22s\n",
      "epoch 12 | loss: 0.44264 | val_0_accuracy: 0.78116 |  0:00:23s\n",
      "epoch 13 | loss: 0.43543 | val_0_accuracy: 0.78213 |  0:00:25s\n",
      "epoch 14 | loss: 0.42965 | val_0_accuracy: 0.79662 |  0:00:26s\n",
      "epoch 15 | loss: 0.42717 | val_0_accuracy: 0.78696 |  0:00:28s\n",
      "epoch 16 | loss: 0.4226  | val_0_accuracy: 0.79082 |  0:00:31s\n",
      "epoch 17 | loss: 0.4273  | val_0_accuracy: 0.79517 |  0:00:33s\n",
      "epoch 18 | loss: 0.41902 | val_0_accuracy: 0.78261 |  0:00:35s\n",
      "epoch 19 | loss: 0.41712 | val_0_accuracy: 0.79662 |  0:00:38s\n",
      "epoch 20 | loss: 0.41105 | val_0_accuracy: 0.79758 |  0:00:41s\n",
      "epoch 21 | loss: 0.41352 | val_0_accuracy: 0.79324 |  0:00:44s\n",
      "epoch 22 | loss: 0.40197 | val_0_accuracy: 0.80628 |  0:00:46s\n",
      "epoch 23 | loss: 0.40319 | val_0_accuracy: 0.79855 |  0:00:49s\n",
      "epoch 24 | loss: 0.39729 | val_0_accuracy: 0.80725 |  0:00:52s\n",
      "epoch 25 | loss: 0.39381 | val_0_accuracy: 0.79758 |  0:00:54s\n",
      "epoch 26 | loss: 0.39342 | val_0_accuracy: 0.8058  |  0:00:57s\n",
      "epoch 27 | loss: 0.39182 | val_0_accuracy: 0.7971  |  0:00:59s\n",
      "epoch 28 | loss: 0.38869 | val_0_accuracy: 0.79662 |  0:01:02s\n",
      "epoch 29 | loss: 0.3851  | val_0_accuracy: 0.80435 |  0:01:04s\n",
      "epoch 30 | loss: 0.38717 | val_0_accuracy: 0.8087  |  0:01:06s\n",
      "epoch 31 | loss: 0.38057 | val_0_accuracy: 0.80193 |  0:01:09s\n",
      "epoch 32 | loss: 0.37749 | val_0_accuracy: 0.80483 |  0:01:12s\n",
      "epoch 33 | loss: 0.38091 | val_0_accuracy: 0.79275 |  0:01:14s\n",
      "epoch 34 | loss: 0.3701  | val_0_accuracy: 0.79807 |  0:01:16s\n",
      "epoch 35 | loss: 0.36787 | val_0_accuracy: 0.79565 |  0:01:19s\n",
      "epoch 36 | loss: 0.36688 | val_0_accuracy: 0.80048 |  0:01:21s\n",
      "epoch 37 | loss: 0.36474 | val_0_accuracy: 0.80193 |  0:01:23s\n",
      "epoch 38 | loss: 0.36712 | val_0_accuracy: 0.79758 |  0:01:25s\n",
      "epoch 39 | loss: 0.3634  | val_0_accuracy: 0.80242 |  0:01:27s\n",
      "epoch 40 | loss: 0.36456 | val_0_accuracy: 0.80676 |  0:01:30s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_accuracy = 0.8087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 19:05:31,423] Trial 7 finished with value: 0.808695652173913 and parameters: {'n_d': 30, 'n_a': 15, 'n_steps': 3, 'gamma': 1.6317190077030883, 'lambda_sparse': 0.004874839157031982, 'lr': 0.0018849505923335876, 'mask_type': 'entmax', 'n_shared': 2, 'n_independent': 1}. Best is trial 3 with value: 0.81256038647343.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78311 | val_0_accuracy: 0.53382 |  0:00:04s\n",
      "epoch 1  | loss: 0.59601 | val_0_accuracy: 0.68937 |  0:00:09s\n",
      "epoch 2  | loss: 0.55735 | val_0_accuracy: 0.72222 |  0:00:13s\n",
      "epoch 3  | loss: 0.53768 | val_0_accuracy: 0.7372  |  0:00:18s\n",
      "epoch 4  | loss: 0.51262 | val_0_accuracy: 0.75362 |  0:00:23s\n",
      "epoch 5  | loss: 0.51974 | val_0_accuracy: 0.76425 |  0:00:28s\n",
      "epoch 6  | loss: 0.49723 | val_0_accuracy: 0.7599  |  0:00:33s\n",
      "epoch 7  | loss: 0.50137 | val_0_accuracy: 0.75266 |  0:00:39s\n",
      "epoch 8  | loss: 0.50238 | val_0_accuracy: 0.75749 |  0:00:44s\n",
      "epoch 9  | loss: 0.49053 | val_0_accuracy: 0.78841 |  0:00:50s\n",
      "epoch 10 | loss: 0.48808 | val_0_accuracy: 0.78696 |  0:00:55s\n",
      "epoch 11 | loss: 0.47967 | val_0_accuracy: 0.77053 |  0:01:00s\n",
      "epoch 12 | loss: 0.47303 | val_0_accuracy: 0.79469 |  0:01:05s\n",
      "epoch 13 | loss: 0.46692 | val_0_accuracy: 0.78937 |  0:01:09s\n",
      "epoch 14 | loss: 0.47125 | val_0_accuracy: 0.78986 |  0:01:14s\n",
      "epoch 15 | loss: 0.46206 | val_0_accuracy: 0.78068 |  0:01:18s\n",
      "epoch 16 | loss: 0.45852 | val_0_accuracy: 0.78889 |  0:01:23s\n",
      "epoch 17 | loss: 0.46557 | val_0_accuracy: 0.78454 |  0:01:27s\n",
      "epoch 18 | loss: 0.46192 | val_0_accuracy: 0.77681 |  0:01:32s\n",
      "epoch 19 | loss: 0.45949 | val_0_accuracy: 0.8     |  0:01:37s\n",
      "epoch 20 | loss: 0.45011 | val_0_accuracy: 0.79034 |  0:01:42s\n",
      "epoch 21 | loss: 0.45751 | val_0_accuracy: 0.79275 |  0:01:46s\n",
      "epoch 22 | loss: 0.45266 | val_0_accuracy: 0.79469 |  0:01:52s\n",
      "epoch 23 | loss: 0.45005 | val_0_accuracy: 0.78406 |  0:01:57s\n",
      "epoch 24 | loss: 0.44493 | val_0_accuracy: 0.79034 |  0:02:03s\n",
      "epoch 25 | loss: 0.44268 | val_0_accuracy: 0.79903 |  0:02:08s\n",
      "epoch 26 | loss: 0.44734 | val_0_accuracy: 0.80048 |  0:02:13s\n",
      "epoch 27 | loss: 0.44709 | val_0_accuracy: 0.8058  |  0:02:18s\n",
      "epoch 28 | loss: 0.44569 | val_0_accuracy: 0.79903 |  0:02:23s\n",
      "epoch 29 | loss: 0.43964 | val_0_accuracy: 0.79469 |  0:02:28s\n",
      "epoch 30 | loss: 0.43428 | val_0_accuracy: 0.8029  |  0:02:33s\n",
      "epoch 31 | loss: 0.43702 | val_0_accuracy: 0.80048 |  0:02:39s\n",
      "epoch 32 | loss: 0.43356 | val_0_accuracy: 0.80676 |  0:02:43s\n",
      "epoch 33 | loss: 0.43266 | val_0_accuracy: 0.80676 |  0:02:48s\n",
      "epoch 34 | loss: 0.43423 | val_0_accuracy: 0.8087  |  0:02:52s\n",
      "epoch 35 | loss: 0.44194 | val_0_accuracy: 0.80966 |  0:02:57s\n",
      "epoch 36 | loss: 0.43653 | val_0_accuracy: 0.7942  |  0:03:01s\n",
      "epoch 37 | loss: 0.43022 | val_0_accuracy: 0.8029  |  0:03:06s\n",
      "epoch 38 | loss: 0.43104 | val_0_accuracy: 0.8029  |  0:03:11s\n",
      "epoch 39 | loss: 0.42131 | val_0_accuracy: 0.80386 |  0:03:15s\n",
      "epoch 40 | loss: 0.41583 | val_0_accuracy: 0.79952 |  0:03:19s\n",
      "epoch 41 | loss: 0.42739 | val_0_accuracy: 0.80773 |  0:03:24s\n",
      "epoch 42 | loss: 0.42262 | val_0_accuracy: 0.80628 |  0:03:29s\n",
      "epoch 43 | loss: 0.42657 | val_0_accuracy: 0.81546 |  0:03:33s\n",
      "epoch 44 | loss: 0.42693 | val_0_accuracy: 0.8029  |  0:03:38s\n",
      "epoch 45 | loss: 0.42284 | val_0_accuracy: 0.8087  |  0:03:43s\n",
      "epoch 46 | loss: 0.42607 | val_0_accuracy: 0.80242 |  0:03:48s\n",
      "epoch 47 | loss: 0.42821 | val_0_accuracy: 0.80821 |  0:03:52s\n",
      "epoch 48 | loss: 0.42348 | val_0_accuracy: 0.80338 |  0:03:56s\n",
      "epoch 49 | loss: 0.42403 | val_0_accuracy: 0.8087  |  0:04:00s\n",
      "epoch 50 | loss: 0.42646 | val_0_accuracy: 0.79662 |  0:04:03s\n",
      "epoch 51 | loss: 0.42286 | val_0_accuracy: 0.80531 |  0:04:06s\n",
      "epoch 52 | loss: 0.41519 | val_0_accuracy: 0.81401 |  0:04:09s\n",
      "epoch 53 | loss: 0.41477 | val_0_accuracy: 0.81208 |  0:04:13s\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_accuracy = 0.81546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 19:09:48,581] Trial 8 finished with value: 0.8154589371980676 and parameters: {'n_d': 25, 'n_a': 30, 'n_steps': 4, 'gamma': 1.971946952264732, 'lambda_sparse': 0.0007485402043618457, 'lr': 0.002788528289062548, 'mask_type': 'entmax', 'n_shared': 3, 'n_independent': 3}. Best is trial 8 with value: 0.8154589371980676.\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.75241 | val_0_accuracy: 0.65314 |  0:00:02s\n",
      "epoch 1  | loss: 0.52849 | val_0_accuracy: 0.72174 |  0:00:03s\n",
      "epoch 2  | loss: 0.496   | val_0_accuracy: 0.74783 |  0:00:06s\n",
      "epoch 3  | loss: 0.48075 | val_0_accuracy: 0.757   |  0:00:08s\n",
      "epoch 4  | loss: 0.46747 | val_0_accuracy: 0.77295 |  0:00:10s\n",
      "epoch 5  | loss: 0.45471 | val_0_accuracy: 0.78019 |  0:00:13s\n",
      "epoch 6  | loss: 0.4387  | val_0_accuracy: 0.79227 |  0:00:15s\n",
      "epoch 7  | loss: 0.42798 | val_0_accuracy: 0.77826 |  0:00:18s\n",
      "epoch 8  | loss: 0.43432 | val_0_accuracy: 0.79275 |  0:00:20s\n",
      "epoch 9  | loss: 0.42482 | val_0_accuracy: 0.79855 |  0:00:23s\n",
      "epoch 10 | loss: 0.41913 | val_0_accuracy: 0.79758 |  0:00:25s\n",
      "epoch 11 | loss: 0.42362 | val_0_accuracy: 0.80145 |  0:00:28s\n",
      "epoch 12 | loss: 0.41952 | val_0_accuracy: 0.80628 |  0:00:30s\n",
      "epoch 13 | loss: 0.41393 | val_0_accuracy: 0.80531 |  0:00:33s\n",
      "epoch 14 | loss: 0.41389 | val_0_accuracy: 0.81353 |  0:00:35s\n",
      "epoch 15 | loss: 0.40376 | val_0_accuracy: 0.81498 |  0:00:38s\n",
      "epoch 16 | loss: 0.39984 | val_0_accuracy: 0.81256 |  0:00:40s\n",
      "epoch 17 | loss: 0.39231 | val_0_accuracy: 0.82126 |  0:00:43s\n",
      "epoch 18 | loss: 0.38129 | val_0_accuracy: 0.82367 |  0:00:45s\n",
      "epoch 19 | loss: 0.3876  | val_0_accuracy: 0.80048 |  0:00:48s\n",
      "epoch 20 | loss: 0.39713 | val_0_accuracy: 0.81063 |  0:00:50s\n",
      "epoch 21 | loss: 0.38035 | val_0_accuracy: 0.82367 |  0:00:53s\n",
      "epoch 22 | loss: 0.37996 | val_0_accuracy: 0.80725 |  0:00:55s\n",
      "epoch 23 | loss: 0.37927 | val_0_accuracy: 0.80918 |  0:00:58s\n",
      "epoch 24 | loss: 0.3846  | val_0_accuracy: 0.82222 |  0:01:00s\n",
      "epoch 25 | loss: 0.37845 | val_0_accuracy: 0.81836 |  0:01:03s\n",
      "epoch 26 | loss: 0.37789 | val_0_accuracy: 0.82126 |  0:01:05s\n",
      "epoch 27 | loss: 0.37766 | val_0_accuracy: 0.81594 |  0:01:08s\n",
      "epoch 28 | loss: 0.37243 | val_0_accuracy: 0.82029 |  0:01:10s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_accuracy = 0.82367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-07 19:11:00,739] Trial 9 finished with value: 0.8236714975845411 and parameters: {'n_d': 56, 'n_a': 50, 'n_steps': 3, 'gamma': 1.7738473640154637, 'lambda_sparse': 0.005413185480278897, 'lr': 0.008417007144643893, 'mask_type': 'entmax', 'n_shared': 1, 'n_independent': 3}. Best is trial 9 with value: 0.8236714975845411.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_d': 56, 'n_a': 50, 'n_steps': 3, 'gamma': 1.7738473640154637, 'lambda_sparse': 0.005413185480278897, 'lr': 0.008417007144643893, 'mask_type': 'entmax', 'n_shared': 1, 'n_independent': 3}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter optimization function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters\n",
    "    params = {\n",
    "        'n_d': trial.suggest_int('n_d', 8, 64),\n",
    "        'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 0.0001, 0.01),\n",
    "        'optimizer_params': {'lr': trial.suggest_float('lr', 1e-4, 1e-2)},  # Set learning rate inside optimizer_params\n",
    "        'mask_type': trial.suggest_categorical('mask_type', ['entmax', 'sparsemax']),\n",
    "        'n_shared': trial.suggest_int('n_shared', 1, 3),\n",
    "        'n_independent': trial.suggest_int('n_independent', 1, 3)\n",
    "    }\n",
    "    \n",
    "    # Initialize TabNetClassifier with the parameters\n",
    "    tabnet_clf = TabNetClassifier(\n",
    "        n_d=params['n_d'],\n",
    "        n_a=params['n_a'],\n",
    "        n_steps=params['n_steps'],\n",
    "        gamma=params['gamma'],\n",
    "        lambda_sparse=params['lambda_sparse'],\n",
    "        optimizer_params=params['optimizer_params'],\n",
    "        mask_type=params['mask_type'],\n",
    "        n_shared=params['n_shared'],\n",
    "        n_independent=params['n_independent']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    tabnet_clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=256,\n",
    "        virtual_batch_size=128\n",
    "    )\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = tabnet_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a94a0489-3743-4319-9ef9-e6631c15a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.14022 |  0:00:01s\n",
      "epoch 1  | loss: 0.57973 |  0:00:02s\n",
      "epoch 2  | loss: 0.52046 |  0:00:03s\n",
      "epoch 3  | loss: 0.4879  |  0:00:04s\n",
      "epoch 4  | loss: 0.46895 |  0:00:04s\n",
      "epoch 5  | loss: 0.45676 |  0:00:05s\n",
      "epoch 6  | loss: 0.45219 |  0:00:06s\n",
      "epoch 7  | loss: 0.43486 |  0:00:07s\n",
      "epoch 8  | loss: 0.42348 |  0:00:08s\n",
      "epoch 9  | loss: 0.42023 |  0:00:09s\n",
      "epoch 10 | loss: 0.41318 |  0:00:09s\n",
      "epoch 11 | loss: 0.40969 |  0:00:10s\n",
      "epoch 12 | loss: 0.39724 |  0:00:11s\n",
      "epoch 13 | loss: 0.39585 |  0:00:12s\n",
      "epoch 14 | loss: 0.38526 |  0:00:13s\n",
      "epoch 15 | loss: 0.38526 |  0:00:15s\n",
      "epoch 16 | loss: 0.37619 |  0:00:16s\n",
      "epoch 17 | loss: 0.37946 |  0:00:17s\n",
      "epoch 18 | loss: 0.36892 |  0:00:19s\n",
      "epoch 19 | loss: 0.36855 |  0:00:20s\n",
      "epoch 20 | loss: 0.36446 |  0:00:21s\n",
      "epoch 21 | loss: 0.35831 |  0:00:22s\n",
      "epoch 22 | loss: 0.35687 |  0:00:23s\n",
      "epoch 23 | loss: 0.34799 |  0:00:25s\n",
      "epoch 24 | loss: 0.34914 |  0:00:26s\n",
      "epoch 25 | loss: 0.34383 |  0:00:27s\n",
      "epoch 26 | loss: 0.32719 |  0:00:29s\n",
      "epoch 27 | loss: 0.32467 |  0:00:30s\n",
      "epoch 28 | loss: 0.3279  |  0:00:31s\n",
      "epoch 29 | loss: 0.3224  |  0:00:33s\n",
      "epoch 30 | loss: 0.32688 |  0:00:34s\n",
      "epoch 31 | loss: 0.3209  |  0:00:35s\n",
      "epoch 32 | loss: 0.31403 |  0:00:37s\n",
      "epoch 33 | loss: 0.31109 |  0:00:39s\n",
      "epoch 34 | loss: 0.31096 |  0:00:40s\n",
      "epoch 35 | loss: 0.31099 |  0:00:41s\n",
      "epoch 36 | loss: 0.29486 |  0:00:42s\n",
      "epoch 37 | loss: 0.29997 |  0:00:43s\n",
      "epoch 38 | loss: 0.30048 |  0:00:44s\n",
      "epoch 39 | loss: 0.29545 |  0:00:46s\n",
      "epoch 40 | loss: 0.28853 |  0:00:47s\n",
      "epoch 41 | loss: 0.28566 |  0:00:48s\n",
      "epoch 42 | loss: 0.27873 |  0:00:49s\n",
      "epoch 43 | loss: 0.27058 |  0:00:51s\n",
      "epoch 44 | loss: 0.26597 |  0:00:52s\n",
      "epoch 45 | loss: 0.25804 |  0:00:53s\n",
      "epoch 46 | loss: 0.26315 |  0:00:54s\n",
      "epoch 47 | loss: 0.2587  |  0:00:55s\n",
      "epoch 48 | loss: 0.25728 |  0:00:56s\n",
      "epoch 49 | loss: 0.2527  |  0:00:58s\n",
      "epoch 50 | loss: 0.25132 |  0:00:59s\n",
      "epoch 51 | loss: 0.25294 |  0:01:00s\n",
      "epoch 52 | loss: 0.24292 |  0:01:01s\n",
      "epoch 53 | loss: 0.24022 |  0:01:02s\n",
      "epoch 54 | loss: 0.23577 |  0:01:03s\n",
      "epoch 55 | loss: 0.23676 |  0:01:05s\n",
      "epoch 56 | loss: 0.23037 |  0:01:06s\n",
      "epoch 57 | loss: 0.23572 |  0:01:07s\n",
      "epoch 58 | loss: 0.2306  |  0:01:08s\n",
      "epoch 59 | loss: 0.23349 |  0:01:09s\n",
      "epoch 60 | loss: 0.22909 |  0:01:10s\n",
      "epoch 61 | loss: 0.22507 |  0:01:11s\n",
      "epoch 62 | loss: 0.21625 |  0:01:13s\n",
      "epoch 63 | loss: 0.21965 |  0:01:14s\n",
      "epoch 64 | loss: 0.22359 |  0:01:15s\n",
      "epoch 65 | loss: 0.20566 |  0:01:16s\n",
      "epoch 66 | loss: 0.2069  |  0:01:17s\n",
      "epoch 67 | loss: 0.1986  |  0:01:18s\n",
      "epoch 68 | loss: 0.20201 |  0:01:20s\n",
      "epoch 69 | loss: 0.20288 |  0:01:21s\n",
      "epoch 70 | loss: 0.20331 |  0:01:22s\n",
      "epoch 71 | loss: 0.19488 |  0:01:24s\n",
      "epoch 72 | loss: 0.20367 |  0:01:25s\n",
      "epoch 73 | loss: 0.1973  |  0:01:26s\n",
      "epoch 74 | loss: 0.18901 |  0:01:27s\n",
      "epoch 75 | loss: 0.19538 |  0:01:28s\n",
      "epoch 76 | loss: 0.1895  |  0:01:30s\n",
      "epoch 77 | loss: 0.19927 |  0:01:31s\n",
      "epoch 78 | loss: 0.19623 |  0:01:32s\n",
      "epoch 79 | loss: 0.19054 |  0:01:33s\n",
      "epoch 80 | loss: 0.18253 |  0:01:34s\n",
      "epoch 81 | loss: 0.1846  |  0:01:35s\n",
      "epoch 82 | loss: 0.18426 |  0:01:37s\n",
      "epoch 83 | loss: 0.17607 |  0:01:38s\n",
      "epoch 84 | loss: 0.17632 |  0:01:39s\n",
      "epoch 85 | loss: 0.17084 |  0:01:40s\n",
      "epoch 86 | loss: 0.16848 |  0:01:42s\n",
      "epoch 87 | loss: 0.17519 |  0:01:43s\n",
      "epoch 88 | loss: 0.17263 |  0:01:44s\n",
      "epoch 89 | loss: 0.17362 |  0:01:45s\n",
      "epoch 90 | loss: 0.17232 |  0:01:46s\n",
      "epoch 91 | loss: 0.18058 |  0:01:47s\n",
      "epoch 92 | loss: 0.1726  |  0:01:49s\n",
      "epoch 93 | loss: 0.16338 |  0:01:50s\n",
      "epoch 94 | loss: 0.16908 |  0:01:51s\n",
      "epoch 95 | loss: 0.16697 |  0:01:52s\n",
      "epoch 96 | loss: 0.15101 |  0:01:54s\n",
      "epoch 97 | loss: 0.15733 |  0:01:55s\n",
      "epoch 98 | loss: 0.16116 |  0:01:57s\n",
      "epoch 99 | loss: 0.15287 |  0:01:58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.12742 |  0:00:01s\n",
      "epoch 1  | loss: 0.58407 |  0:00:03s\n",
      "epoch 2  | loss: 0.51794 |  0:00:04s\n",
      "epoch 3  | loss: 0.48681 |  0:00:06s\n",
      "epoch 4  | loss: 0.46947 |  0:00:07s\n",
      "epoch 5  | loss: 0.45933 |  0:00:08s\n",
      "epoch 6  | loss: 0.44599 |  0:00:10s\n",
      "epoch 7  | loss: 0.43837 |  0:00:11s\n",
      "epoch 8  | loss: 0.42354 |  0:00:13s\n",
      "epoch 9  | loss: 0.42432 |  0:00:14s\n",
      "epoch 10 | loss: 0.41392 |  0:00:16s\n",
      "epoch 11 | loss: 0.41543 |  0:00:17s\n",
      "epoch 12 | loss: 0.41205 |  0:00:18s\n",
      "epoch 13 | loss: 0.41255 |  0:00:19s\n",
      "epoch 14 | loss: 0.40892 |  0:00:20s\n",
      "epoch 15 | loss: 0.40056 |  0:00:22s\n",
      "epoch 16 | loss: 0.39638 |  0:00:23s\n",
      "epoch 17 | loss: 0.38054 |  0:00:25s\n",
      "epoch 18 | loss: 0.37647 |  0:00:27s\n",
      "epoch 19 | loss: 0.37048 |  0:00:28s\n",
      "epoch 20 | loss: 0.36521 |  0:00:30s\n",
      "epoch 21 | loss: 0.35904 |  0:00:31s\n",
      "epoch 22 | loss: 0.36881 |  0:00:32s\n",
      "epoch 23 | loss: 0.35857 |  0:00:34s\n",
      "epoch 24 | loss: 0.36185 |  0:00:35s\n",
      "epoch 25 | loss: 0.3535  |  0:00:36s\n",
      "epoch 26 | loss: 0.35157 |  0:00:37s\n",
      "epoch 27 | loss: 0.34875 |  0:00:38s\n",
      "epoch 28 | loss: 0.33927 |  0:00:39s\n",
      "epoch 29 | loss: 0.3376  |  0:00:40s\n",
      "epoch 30 | loss: 0.33547 |  0:00:41s\n",
      "epoch 31 | loss: 0.33749 |  0:00:43s\n",
      "epoch 32 | loss: 0.32439 |  0:00:44s\n",
      "epoch 33 | loss: 0.3232  |  0:00:45s\n",
      "epoch 34 | loss: 0.32679 |  0:00:46s\n",
      "epoch 35 | loss: 0.31778 |  0:00:48s\n",
      "epoch 36 | loss: 0.30383 |  0:00:49s\n",
      "epoch 37 | loss: 0.30533 |  0:00:50s\n",
      "epoch 38 | loss: 0.30247 |  0:00:51s\n",
      "epoch 39 | loss: 0.29036 |  0:00:53s\n",
      "epoch 40 | loss: 0.29725 |  0:00:54s\n",
      "epoch 41 | loss: 0.2945  |  0:00:55s\n",
      "epoch 42 | loss: 0.29133 |  0:00:56s\n",
      "epoch 43 | loss: 0.28289 |  0:00:57s\n",
      "epoch 44 | loss: 0.27298 |  0:00:58s\n",
      "epoch 45 | loss: 0.26879 |  0:00:59s\n",
      "epoch 46 | loss: 0.27121 |  0:01:00s\n",
      "epoch 47 | loss: 0.26642 |  0:01:01s\n",
      "epoch 48 | loss: 0.26363 |  0:01:02s\n",
      "epoch 49 | loss: 0.26357 |  0:01:03s\n",
      "epoch 50 | loss: 0.26511 |  0:01:04s\n",
      "epoch 51 | loss: 0.25853 |  0:01:06s\n",
      "epoch 52 | loss: 0.2483  |  0:01:07s\n",
      "epoch 53 | loss: 0.24513 |  0:01:08s\n",
      "epoch 54 | loss: 0.24474 |  0:01:09s\n",
      "epoch 55 | loss: 0.24475 |  0:01:10s\n",
      "epoch 56 | loss: 0.23906 |  0:01:11s\n",
      "epoch 57 | loss: 0.23843 |  0:01:12s\n",
      "epoch 58 | loss: 0.22787 |  0:01:13s\n",
      "epoch 59 | loss: 0.22423 |  0:01:14s\n",
      "epoch 60 | loss: 0.22888 |  0:01:15s\n",
      "epoch 61 | loss: 0.22575 |  0:01:16s\n",
      "epoch 62 | loss: 0.2234  |  0:01:18s\n",
      "epoch 63 | loss: 0.22125 |  0:01:19s\n",
      "epoch 64 | loss: 0.21262 |  0:01:20s\n",
      "epoch 65 | loss: 0.20662 |  0:01:21s\n",
      "epoch 66 | loss: 0.21187 |  0:01:22s\n",
      "epoch 67 | loss: 0.20999 |  0:01:23s\n",
      "epoch 68 | loss: 0.1999  |  0:01:25s\n",
      "epoch 69 | loss: 0.20183 |  0:01:30s\n",
      "epoch 70 | loss: 0.1982  |  0:01:31s\n",
      "epoch 71 | loss: 0.19668 |  0:01:33s\n",
      "epoch 72 | loss: 0.19189 |  0:01:35s\n",
      "epoch 73 | loss: 0.19981 |  0:01:36s\n",
      "epoch 74 | loss: 0.19081 |  0:01:38s\n",
      "epoch 75 | loss: 0.19138 |  0:01:40s\n",
      "epoch 76 | loss: 0.1869  |  0:01:41s\n",
      "epoch 77 | loss: 0.19286 |  0:01:43s\n",
      "epoch 78 | loss: 0.19005 |  0:01:44s\n",
      "epoch 79 | loss: 0.18634 |  0:01:45s\n",
      "epoch 80 | loss: 0.18155 |  0:01:46s\n",
      "epoch 81 | loss: 0.1795  |  0:01:47s\n",
      "epoch 82 | loss: 0.1757  |  0:01:48s\n",
      "epoch 83 | loss: 0.17852 |  0:01:49s\n",
      "epoch 84 | loss: 0.17826 |  0:01:50s\n",
      "epoch 85 | loss: 0.16824 |  0:01:51s\n",
      "epoch 86 | loss: 0.17325 |  0:01:53s\n",
      "epoch 87 | loss: 0.17127 |  0:01:54s\n",
      "epoch 88 | loss: 0.1631  |  0:01:55s\n",
      "epoch 89 | loss: 0.1628  |  0:01:56s\n",
      "epoch 90 | loss: 0.16713 |  0:01:57s\n",
      "epoch 91 | loss: 0.17346 |  0:01:58s\n",
      "epoch 92 | loss: 0.17678 |  0:01:59s\n",
      "epoch 93 | loss: 0.17738 |  0:02:00s\n",
      "epoch 94 | loss: 0.17999 |  0:02:02s\n",
      "epoch 95 | loss: 0.1734  |  0:02:03s\n",
      "epoch 96 | loss: 0.16694 |  0:02:04s\n",
      "epoch 97 | loss: 0.16759 |  0:02:05s\n",
      "epoch 98 | loss: 0.16822 |  0:02:06s\n",
      "epoch 99 | loss: 0.16428 |  0:02:07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.12104 |  0:00:02s\n",
      "epoch 1  | loss: 0.58736 |  0:00:05s\n",
      "epoch 2  | loss: 0.52709 |  0:00:07s\n",
      "epoch 3  | loss: 0.49788 |  0:00:09s\n",
      "epoch 4  | loss: 0.47901 |  0:00:11s\n",
      "epoch 5  | loss: 0.46644 |  0:00:14s\n",
      "epoch 6  | loss: 0.45243 |  0:00:16s\n",
      "epoch 7  | loss: 0.44479 |  0:00:19s\n",
      "epoch 8  | loss: 0.43515 |  0:00:21s\n",
      "epoch 9  | loss: 0.42203 |  0:00:24s\n",
      "epoch 10 | loss: 0.42479 |  0:00:26s\n",
      "epoch 11 | loss: 0.41894 |  0:00:28s\n",
      "epoch 12 | loss: 0.42327 |  0:00:29s\n",
      "epoch 13 | loss: 0.42036 |  0:00:31s\n",
      "epoch 14 | loss: 0.40611 |  0:00:32s\n",
      "epoch 15 | loss: 0.39827 |  0:00:33s\n",
      "epoch 16 | loss: 0.40195 |  0:00:35s\n",
      "epoch 17 | loss: 0.39689 |  0:00:36s\n",
      "epoch 18 | loss: 0.39337 |  0:00:38s\n",
      "epoch 19 | loss: 0.38474 |  0:00:39s\n",
      "epoch 20 | loss: 0.38063 |  0:00:40s\n",
      "epoch 21 | loss: 0.37323 |  0:00:42s\n",
      "epoch 22 | loss: 0.37107 |  0:00:43s\n",
      "epoch 23 | loss: 0.35884 |  0:00:44s\n",
      "epoch 24 | loss: 0.35876 |  0:00:46s\n",
      "epoch 25 | loss: 0.35346 |  0:00:47s\n",
      "epoch 26 | loss: 0.36215 |  0:00:48s\n",
      "epoch 27 | loss: 0.35149 |  0:00:50s\n",
      "epoch 28 | loss: 0.35382 |  0:00:51s\n",
      "epoch 29 | loss: 0.35023 |  0:00:52s\n",
      "epoch 30 | loss: 0.34761 |  0:00:53s\n",
      "epoch 31 | loss: 0.33613 |  0:00:55s\n",
      "epoch 32 | loss: 0.33417 |  0:00:56s\n",
      "epoch 33 | loss: 0.32657 |  0:00:58s\n",
      "epoch 34 | loss: 0.33364 |  0:00:59s\n",
      "epoch 35 | loss: 0.3349  |  0:01:00s\n",
      "epoch 36 | loss: 0.33183 |  0:01:01s\n",
      "epoch 37 | loss: 0.32837 |  0:01:03s\n",
      "epoch 38 | loss: 0.32116 |  0:01:04s\n",
      "epoch 39 | loss: 0.31645 |  0:01:05s\n",
      "epoch 40 | loss: 0.30971 |  0:01:07s\n",
      "epoch 41 | loss: 0.30773 |  0:01:09s\n",
      "epoch 42 | loss: 0.30426 |  0:01:10s\n",
      "epoch 43 | loss: 0.30048 |  0:01:11s\n",
      "epoch 44 | loss: 0.29949 |  0:01:12s\n",
      "epoch 45 | loss: 0.29582 |  0:01:14s\n",
      "epoch 46 | loss: 0.29155 |  0:01:15s\n",
      "epoch 47 | loss: 0.28885 |  0:01:16s\n",
      "epoch 48 | loss: 0.28802 |  0:01:18s\n",
      "epoch 49 | loss: 0.27776 |  0:01:19s\n",
      "epoch 50 | loss: 0.2843  |  0:01:21s\n",
      "epoch 51 | loss: 0.28052 |  0:01:22s\n",
      "epoch 52 | loss: 0.27331 |  0:01:23s\n",
      "epoch 53 | loss: 0.26129 |  0:01:25s\n",
      "epoch 54 | loss: 0.25884 |  0:01:26s\n",
      "epoch 55 | loss: 0.26126 |  0:01:27s\n",
      "epoch 56 | loss: 0.2547  |  0:01:29s\n",
      "epoch 57 | loss: 0.25    |  0:01:30s\n",
      "epoch 58 | loss: 0.25036 |  0:01:31s\n",
      "epoch 59 | loss: 0.24698 |  0:01:32s\n",
      "epoch 60 | loss: 0.2485  |  0:01:34s\n",
      "epoch 61 | loss: 0.23971 |  0:01:35s\n",
      "epoch 62 | loss: 0.22854 |  0:01:36s\n",
      "epoch 63 | loss: 0.23574 |  0:01:37s\n",
      "epoch 64 | loss: 0.23417 |  0:01:39s\n",
      "epoch 65 | loss: 0.23148 |  0:01:40s\n",
      "epoch 66 | loss: 0.22509 |  0:01:41s\n",
      "epoch 67 | loss: 0.22662 |  0:01:42s\n",
      "epoch 68 | loss: 0.2254  |  0:01:44s\n",
      "epoch 69 | loss: 0.22055 |  0:01:45s\n",
      "epoch 70 | loss: 0.21625 |  0:01:47s\n",
      "epoch 71 | loss: 0.21589 |  0:01:48s\n",
      "epoch 72 | loss: 0.21729 |  0:01:49s\n",
      "epoch 73 | loss: 0.21126 |  0:01:50s\n",
      "epoch 74 | loss: 0.20519 |  0:01:51s\n",
      "epoch 75 | loss: 0.20001 |  0:01:53s\n",
      "epoch 76 | loss: 0.18909 |  0:01:54s\n",
      "epoch 77 | loss: 0.19333 |  0:01:55s\n",
      "epoch 78 | loss: 0.18653 |  0:01:56s\n",
      "epoch 79 | loss: 0.18624 |  0:01:58s\n",
      "epoch 80 | loss: 0.19237 |  0:01:59s\n",
      "epoch 81 | loss: 0.1897  |  0:02:00s\n",
      "epoch 82 | loss: 0.1898  |  0:02:02s\n",
      "epoch 83 | loss: 0.19379 |  0:02:04s\n",
      "epoch 84 | loss: 0.19131 |  0:02:05s\n",
      "epoch 85 | loss: 0.18262 |  0:02:07s\n",
      "epoch 86 | loss: 0.1831  |  0:02:08s\n",
      "epoch 87 | loss: 0.17425 |  0:02:10s\n",
      "epoch 88 | loss: 0.18095 |  0:02:11s\n",
      "epoch 89 | loss: 0.17856 |  0:02:13s\n",
      "epoch 90 | loss: 0.19149 |  0:02:14s\n",
      "epoch 91 | loss: 0.18304 |  0:02:15s\n",
      "epoch 92 | loss: 0.17313 |  0:02:17s\n",
      "epoch 93 | loss: 0.16865 |  0:02:18s\n",
      "epoch 94 | loss: 0.16255 |  0:02:20s\n",
      "epoch 95 | loss: 0.15731 |  0:02:21s\n",
      "epoch 96 | loss: 0.15846 |  0:02:22s\n",
      "epoch 97 | loss: 0.16831 |  0:02:23s\n",
      "epoch 98 | loss: 0.15844 |  0:02:25s\n",
      "epoch 99 | loss: 0.17155 |  0:02:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.10564 |  0:00:05s\n",
      "epoch 1  | loss: 0.56305 |  0:00:06s\n",
      "epoch 2  | loss: 0.51644 |  0:00:08s\n",
      "epoch 3  | loss: 0.48879 |  0:00:10s\n",
      "epoch 4  | loss: 0.46642 |  0:00:12s\n",
      "epoch 5  | loss: 0.45855 |  0:00:13s\n",
      "epoch 6  | loss: 0.44481 |  0:00:15s\n",
      "epoch 7  | loss: 0.44039 |  0:00:16s\n",
      "epoch 8  | loss: 0.43542 |  0:00:18s\n",
      "epoch 9  | loss: 0.42846 |  0:00:20s\n",
      "epoch 10 | loss: 0.41297 |  0:00:21s\n",
      "epoch 11 | loss: 0.41296 |  0:00:23s\n",
      "epoch 12 | loss: 0.40515 |  0:00:24s\n",
      "epoch 13 | loss: 0.40054 |  0:00:26s\n",
      "epoch 14 | loss: 0.3939  |  0:00:28s\n",
      "epoch 15 | loss: 0.3889  |  0:00:30s\n",
      "epoch 16 | loss: 0.39025 |  0:00:31s\n",
      "epoch 17 | loss: 0.39103 |  0:00:33s\n",
      "epoch 18 | loss: 0.38946 |  0:00:35s\n",
      "epoch 19 | loss: 0.38296 |  0:00:37s\n",
      "epoch 20 | loss: 0.38473 |  0:00:39s\n",
      "epoch 21 | loss: 0.38866 |  0:00:41s\n",
      "epoch 22 | loss: 0.38651 |  0:00:42s\n",
      "epoch 23 | loss: 0.3813  |  0:00:44s\n",
      "epoch 24 | loss: 0.37321 |  0:00:47s\n",
      "epoch 25 | loss: 0.37445 |  0:00:49s\n",
      "epoch 26 | loss: 0.36517 |  0:00:51s\n",
      "epoch 27 | loss: 0.35831 |  0:00:53s\n",
      "epoch 28 | loss: 0.35071 |  0:00:55s\n",
      "epoch 29 | loss: 0.35012 |  0:00:57s\n",
      "epoch 30 | loss: 0.35234 |  0:00:58s\n",
      "epoch 31 | loss: 0.35335 |  0:01:00s\n",
      "epoch 32 | loss: 0.34219 |  0:01:02s\n",
      "epoch 33 | loss: 0.3465  |  0:01:04s\n",
      "epoch 34 | loss: 0.33922 |  0:01:06s\n",
      "epoch 35 | loss: 0.32967 |  0:01:07s\n",
      "epoch 36 | loss: 0.32152 |  0:01:09s\n",
      "epoch 37 | loss: 0.31516 |  0:01:11s\n",
      "epoch 38 | loss: 0.31163 |  0:01:13s\n",
      "epoch 39 | loss: 0.31463 |  0:01:14s\n",
      "epoch 40 | loss: 0.30793 |  0:01:16s\n",
      "epoch 41 | loss: 0.31175 |  0:01:17s\n",
      "epoch 42 | loss: 0.30765 |  0:01:19s\n",
      "epoch 43 | loss: 0.30428 |  0:01:20s\n",
      "epoch 44 | loss: 0.29671 |  0:01:22s\n",
      "epoch 45 | loss: 0.29749 |  0:01:23s\n",
      "epoch 46 | loss: 0.29047 |  0:01:25s\n",
      "epoch 47 | loss: 0.28398 |  0:01:27s\n",
      "epoch 48 | loss: 0.27922 |  0:01:28s\n",
      "epoch 49 | loss: 0.27369 |  0:01:30s\n",
      "epoch 50 | loss: 0.27534 |  0:01:32s\n",
      "epoch 51 | loss: 0.26928 |  0:01:33s\n",
      "epoch 52 | loss: 0.27237 |  0:01:34s\n",
      "epoch 53 | loss: 0.27008 |  0:01:35s\n",
      "epoch 54 | loss: 0.2657  |  0:01:37s\n",
      "epoch 55 | loss: 0.25015 |  0:01:38s\n",
      "epoch 56 | loss: 0.25091 |  0:01:40s\n",
      "epoch 57 | loss: 0.25039 |  0:01:41s\n",
      "epoch 58 | loss: 0.23931 |  0:01:43s\n",
      "epoch 59 | loss: 0.23512 |  0:01:44s\n",
      "epoch 60 | loss: 0.23281 |  0:01:45s\n",
      "epoch 61 | loss: 0.24405 |  0:01:47s\n",
      "epoch 62 | loss: 0.23561 |  0:01:48s\n",
      "epoch 63 | loss: 0.23503 |  0:01:50s\n",
      "epoch 64 | loss: 0.24208 |  0:01:51s\n",
      "epoch 65 | loss: 0.23655 |  0:01:53s\n",
      "epoch 66 | loss: 0.22805 |  0:01:54s\n",
      "epoch 67 | loss: 0.23423 |  0:01:56s\n",
      "epoch 68 | loss: 0.23245 |  0:01:57s\n",
      "epoch 69 | loss: 0.23838 |  0:01:58s\n",
      "epoch 70 | loss: 0.23822 |  0:01:59s\n",
      "epoch 71 | loss: 0.23281 |  0:02:01s\n",
      "epoch 72 | loss: 0.23056 |  0:02:02s\n",
      "epoch 73 | loss: 0.23437 |  0:02:03s\n",
      "epoch 74 | loss: 0.23728 |  0:02:05s\n",
      "epoch 75 | loss: 0.2234  |  0:02:06s\n",
      "epoch 76 | loss: 0.22283 |  0:02:07s\n",
      "epoch 77 | loss: 0.22071 |  0:02:09s\n",
      "epoch 78 | loss: 0.22514 |  0:02:10s\n",
      "epoch 79 | loss: 0.22199 |  0:02:12s\n",
      "epoch 80 | loss: 0.21105 |  0:02:13s\n",
      "epoch 81 | loss: 0.20798 |  0:02:15s\n",
      "epoch 82 | loss: 0.20408 |  0:02:16s\n",
      "epoch 83 | loss: 0.20894 |  0:02:18s\n",
      "epoch 84 | loss: 0.20437 |  0:02:19s\n",
      "epoch 85 | loss: 0.20256 |  0:02:20s\n",
      "epoch 86 | loss: 0.19514 |  0:02:22s\n",
      "epoch 87 | loss: 0.18842 |  0:02:23s\n",
      "epoch 88 | loss: 0.19135 |  0:02:25s\n",
      "epoch 89 | loss: 0.18994 |  0:02:26s\n",
      "epoch 90 | loss: 0.18365 |  0:02:27s\n",
      "epoch 91 | loss: 0.19859 |  0:02:29s\n",
      "epoch 92 | loss: 0.19581 |  0:02:31s\n",
      "epoch 93 | loss: 0.19049 |  0:02:32s\n",
      "epoch 94 | loss: 0.19151 |  0:02:34s\n",
      "epoch 95 | loss: 0.19216 |  0:02:35s\n",
      "epoch 96 | loss: 0.18575 |  0:02:37s\n",
      "epoch 97 | loss: 0.18124 |  0:02:38s\n",
      "epoch 98 | loss: 0.18267 |  0:02:40s\n",
      "epoch 99 | loss: 0.19113 |  0:02:41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.12544 |  0:00:01s\n",
      "epoch 1  | loss: 0.55245 |  0:00:02s\n",
      "epoch 2  | loss: 0.50675 |  0:00:03s\n",
      "epoch 3  | loss: 0.48214 |  0:00:05s\n",
      "epoch 4  | loss: 0.46208 |  0:00:06s\n",
      "epoch 5  | loss: 0.45412 |  0:00:08s\n",
      "epoch 6  | loss: 0.43959 |  0:00:10s\n",
      "epoch 7  | loss: 0.44062 |  0:00:12s\n",
      "epoch 8  | loss: 0.42525 |  0:00:13s\n",
      "epoch 9  | loss: 0.42373 |  0:00:15s\n",
      "epoch 10 | loss: 0.41165 |  0:00:16s\n",
      "epoch 11 | loss: 0.40097 |  0:00:18s\n",
      "epoch 12 | loss: 0.39586 |  0:00:19s\n",
      "epoch 13 | loss: 0.39224 |  0:00:21s\n",
      "epoch 14 | loss: 0.39192 |  0:00:22s\n",
      "epoch 15 | loss: 0.38773 |  0:00:24s\n",
      "epoch 16 | loss: 0.37523 |  0:00:25s\n",
      "epoch 17 | loss: 0.37765 |  0:00:27s\n",
      "epoch 18 | loss: 0.36493 |  0:00:29s\n",
      "epoch 19 | loss: 0.35977 |  0:00:30s\n",
      "epoch 20 | loss: 0.34915 |  0:00:32s\n",
      "epoch 21 | loss: 0.35324 |  0:00:34s\n",
      "epoch 22 | loss: 0.35521 |  0:00:35s\n",
      "epoch 23 | loss: 0.35263 |  0:00:37s\n",
      "epoch 24 | loss: 0.34735 |  0:00:38s\n",
      "epoch 25 | loss: 0.34313 |  0:00:40s\n",
      "epoch 26 | loss: 0.33619 |  0:00:41s\n",
      "epoch 27 | loss: 0.32966 |  0:00:43s\n",
      "epoch 28 | loss: 0.3323  |  0:00:44s\n",
      "epoch 29 | loss: 0.32783 |  0:00:46s\n",
      "epoch 30 | loss: 0.31855 |  0:00:47s\n",
      "epoch 31 | loss: 0.31391 |  0:00:49s\n",
      "epoch 32 | loss: 0.30869 |  0:00:50s\n",
      "epoch 33 | loss: 0.3049  |  0:00:51s\n",
      "epoch 34 | loss: 0.31016 |  0:00:53s\n",
      "epoch 35 | loss: 0.32862 |  0:00:54s\n",
      "epoch 36 | loss: 0.30921 |  0:00:56s\n",
      "epoch 37 | loss: 0.30691 |  0:00:57s\n",
      "epoch 38 | loss: 0.29912 |  0:00:58s\n",
      "epoch 39 | loss: 0.30898 |  0:00:59s\n",
      "epoch 40 | loss: 0.30395 |  0:01:00s\n",
      "epoch 41 | loss: 0.29291 |  0:01:01s\n",
      "epoch 42 | loss: 0.28065 |  0:01:03s\n",
      "epoch 43 | loss: 0.28735 |  0:01:04s\n",
      "epoch 44 | loss: 0.27822 |  0:01:05s\n",
      "epoch 45 | loss: 0.29093 |  0:01:06s\n",
      "epoch 46 | loss: 0.28192 |  0:01:08s\n",
      "epoch 47 | loss: 0.26984 |  0:01:09s\n",
      "epoch 48 | loss: 0.27994 |  0:01:10s\n",
      "epoch 49 | loss: 0.29    |  0:01:11s\n",
      "epoch 50 | loss: 0.27606 |  0:01:13s\n",
      "epoch 51 | loss: 0.27231 |  0:01:14s\n",
      "epoch 52 | loss: 0.27181 |  0:01:15s\n",
      "epoch 53 | loss: 0.27427 |  0:01:16s\n",
      "epoch 54 | loss: 0.26827 |  0:01:17s\n",
      "epoch 55 | loss: 0.27117 |  0:01:18s\n",
      "epoch 56 | loss: 0.25946 |  0:01:20s\n",
      "epoch 57 | loss: 0.25073 |  0:01:21s\n",
      "epoch 58 | loss: 0.25069 |  0:01:22s\n",
      "epoch 59 | loss: 0.23733 |  0:01:23s\n",
      "epoch 60 | loss: 0.2431  |  0:01:25s\n",
      "epoch 61 | loss: 0.25771 |  0:01:26s\n",
      "epoch 62 | loss: 0.2523  |  0:01:27s\n",
      "epoch 63 | loss: 0.24898 |  0:01:28s\n",
      "epoch 64 | loss: 0.2454  |  0:01:30s\n",
      "epoch 65 | loss: 0.23434 |  0:01:31s\n",
      "epoch 66 | loss: 0.23308 |  0:01:32s\n",
      "epoch 67 | loss: 0.26303 |  0:01:34s\n",
      "epoch 68 | loss: 0.24818 |  0:01:35s\n",
      "epoch 69 | loss: 0.25819 |  0:01:36s\n",
      "epoch 70 | loss: 0.25786 |  0:01:37s\n",
      "epoch 71 | loss: 0.26211 |  0:01:38s\n",
      "epoch 72 | loss: 0.25958 |  0:01:40s\n",
      "epoch 73 | loss: 0.24993 |  0:01:41s\n",
      "epoch 74 | loss: 0.24308 |  0:01:43s\n",
      "epoch 75 | loss: 0.24    |  0:01:44s\n",
      "epoch 76 | loss: 0.23069 |  0:01:45s\n",
      "epoch 77 | loss: 0.22906 |  0:01:46s\n",
      "epoch 78 | loss: 0.22019 |  0:01:47s\n",
      "epoch 79 | loss: 0.22012 |  0:01:48s\n",
      "epoch 80 | loss: 0.22871 |  0:01:50s\n",
      "epoch 81 | loss: 0.2306  |  0:01:51s\n",
      "epoch 82 | loss: 0.22658 |  0:01:52s\n",
      "epoch 83 | loss: 0.22355 |  0:01:53s\n",
      "epoch 84 | loss: 0.21556 |  0:01:54s\n",
      "epoch 85 | loss: 0.20456 |  0:02:00s\n",
      "epoch 86 | loss: 0.20508 |  0:02:03s\n",
      "epoch 87 | loss: 0.19735 |  0:02:05s\n",
      "epoch 88 | loss: 0.19395 |  0:02:07s\n",
      "epoch 89 | loss: 0.18739 |  0:02:09s\n",
      "epoch 90 | loss: 0.1809  |  0:02:11s\n",
      "epoch 91 | loss: 0.18494 |  0:02:12s\n",
      "epoch 92 | loss: 0.18181 |  0:02:14s\n",
      "epoch 93 | loss: 0.18895 |  0:02:16s\n",
      "epoch 94 | loss: 0.19601 |  0:02:18s\n",
      "epoch 95 | loss: 0.18784 |  0:02:19s\n",
      "epoch 96 | loss: 0.18416 |  0:02:21s\n",
      "epoch 97 | loss: 0.17894 |  0:02:23s\n",
      "epoch 98 | loss: 0.17504 |  0:02:24s\n",
      "epoch 99 | loss: 0.16984 |  0:02:26s\n",
      "Cross-Validated Accuracy: 0.8247970622342482\n",
      "epoch 0  | loss: 0.75241 | val_0_accuracy: 0.65314 |  0:00:03s\n",
      "epoch 1  | loss: 0.52849 | val_0_accuracy: 0.72174 |  0:00:06s\n",
      "epoch 2  | loss: 0.496   | val_0_accuracy: 0.74783 |  0:00:09s\n",
      "epoch 3  | loss: 0.48075 | val_0_accuracy: 0.757   |  0:00:14s\n",
      "epoch 4  | loss: 0.46747 | val_0_accuracy: 0.77295 |  0:00:18s\n",
      "epoch 5  | loss: 0.45471 | val_0_accuracy: 0.78019 |  0:00:20s\n",
      "epoch 6  | loss: 0.4387  | val_0_accuracy: 0.79227 |  0:00:23s\n",
      "epoch 7  | loss: 0.42798 | val_0_accuracy: 0.77826 |  0:00:27s\n",
      "epoch 8  | loss: 0.43432 | val_0_accuracy: 0.79275 |  0:00:32s\n",
      "epoch 9  | loss: 0.42482 | val_0_accuracy: 0.79855 |  0:00:36s\n",
      "epoch 10 | loss: 0.41913 | val_0_accuracy: 0.79758 |  0:00:40s\n",
      "epoch 11 | loss: 0.42362 | val_0_accuracy: 0.80145 |  0:00:43s\n",
      "epoch 12 | loss: 0.41952 | val_0_accuracy: 0.80628 |  0:00:46s\n",
      "epoch 13 | loss: 0.41393 | val_0_accuracy: 0.80531 |  0:00:49s\n",
      "epoch 14 | loss: 0.41389 | val_0_accuracy: 0.81353 |  0:00:51s\n",
      "epoch 15 | loss: 0.40376 | val_0_accuracy: 0.81498 |  0:00:53s\n",
      "epoch 16 | loss: 0.39984 | val_0_accuracy: 0.81256 |  0:00:56s\n",
      "epoch 17 | loss: 0.39231 | val_0_accuracy: 0.82126 |  0:00:58s\n",
      "epoch 18 | loss: 0.38129 | val_0_accuracy: 0.82367 |  0:01:00s\n",
      "epoch 19 | loss: 0.3876  | val_0_accuracy: 0.80048 |  0:01:02s\n",
      "epoch 20 | loss: 0.39713 | val_0_accuracy: 0.81063 |  0:01:05s\n",
      "epoch 21 | loss: 0.38035 | val_0_accuracy: 0.82367 |  0:01:07s\n",
      "epoch 22 | loss: 0.37996 | val_0_accuracy: 0.80725 |  0:01:12s\n",
      "epoch 23 | loss: 0.37927 | val_0_accuracy: 0.80918 |  0:01:16s\n",
      "epoch 24 | loss: 0.3846  | val_0_accuracy: 0.82222 |  0:01:21s\n",
      "epoch 25 | loss: 0.37845 | val_0_accuracy: 0.81836 |  0:01:26s\n",
      "epoch 26 | loss: 0.37789 | val_0_accuracy: 0.82126 |  0:01:30s\n",
      "epoch 27 | loss: 0.37766 | val_0_accuracy: 0.81594 |  0:01:33s\n",
      "epoch 28 | loss: 0.37243 | val_0_accuracy: 0.82029 |  0:01:35s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_accuracy = 0.82367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Optimized TabNet:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      1021\n",
      "           1       0.80      0.87      0.83      1049\n",
      "\n",
      "    accuracy                           0.82      2070\n",
      "   macro avg       0.83      0.82      0.82      2070\n",
      "weighted avg       0.83      0.82      0.82      2070\n",
      "\n",
      "Confusion Matrix for Optimized TabNet:\n",
      " [[797 224]\n",
      " [141 908]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TabNet model with the best hyperparameters\n",
    "optimized_tabnet_clf = TabNetClassifier(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_params={'lr': best_params['lr']},  # Correctly pass the learning rate\n",
    "    mask_type=best_params['mask_type'],\n",
    "    n_shared=best_params['n_shared'],\n",
    "    n_independent=best_params['n_independent']\n",
    ")\n",
    "\n",
    "# Perform cross-validation to evaluate model performance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_cv_pred = cross_val_predict(optimized_tabnet_clf, features_resampled, target_resampled, cv=cv)\n",
    "\n",
    "# Calculate cross-validated accuracy\n",
    "cv_accuracy = accuracy_score(target_resampled, y_cv_pred)\n",
    "print(\"Cross-Validated Accuracy:\", cv_accuracy)\n",
    "\n",
    "# Train the optimized model on the full dataset\n",
    "optimized_tabnet_clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "\n",
    "# Make final predictions on the test set\n",
    "y_pred_optimized = optimized_tabnet_clf.predict(X_test)\n",
    "\n",
    "# Calculate the classification report and confusion matrix\n",
    "print(\"Classification Report for Optimized TabNet:\\n\", classification_report(y_test, y_pred_optimized))\n",
    "print(\"Confusion Matrix for Optimized TabNet:\\n\", confusion_matrix(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75984e79-eabf-4e0c-bad6-6409d4bd9b2d",
   "metadata": {},
   "source": [
    "### Understand the Data Processing and Model Development\n",
    "First, let's break down your code to identify the critical steps for preprocessing, training, and optimizing your model:\n",
    "\n",
    "**Data Loading and Preprocessing:**\n",
    "\n",
    "+ Loading the dataset.\n",
    "+ Converting the target variable to numeric.\n",
    "+ Encoding categorical variables using Label Encoding.\n",
    "+ Applying the SMOTE technique for handling class imbalance.\n",
    "\n",
    "**Splitting the Data:**\n",
    "\n",
    "+ Splitting the processed data into training and testing sets.\n",
    "\n",
    "**Hyperparameter Optimization:**\n",
    "\n",
    "+ Using Optuna for hyperparameter tuning of the TabNetClassifier.\n",
    "\n",
    "**Model Training and Evaluation:**\n",
    "\n",
    "+ Training the optimized model using cross-validation.\n",
    "+ Evaluating the model's performance with a classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fa888-1e52-4e3e-9e1d-13f1d3438662",
   "metadata": {},
   "source": [
    "### Save the Preprocessing Objects and Model\n",
    "\n",
    "To deploy the model, save the preprocessing objects (e.g., label encoders, SMOTE object) and the trained model itself. We will use the joblib library for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0362eb83-2583-4fac-a8f8-597a26e329b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb34c2a-1bee-4aeb-8210-ebb463f3ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features in Raw Data : --- (7043, 19)\n",
      "Shape of Targets in Raw Data : --- (7043,)\n",
      "Shape of Features after doing the SMOTE Technique : --- (10348, 19)\n",
      "Shape of Targets after doing the SMOTE Technique : --- (10348,)\n",
      "(8278, 19)\n",
      "(2070, 19)\n",
      "(8278,)\n",
      "(2070,)\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset\n",
    "data = pd.read_csv('no_missing_values_customer_data.csv')\n",
    "\n",
    "## Convert the target variable 'Churn' to numeric\n",
    "data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "## Encode categorical variables using Label Encoding\n",
    "label_encoders = {}  # Dictionary to store label encoders\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    if col != 'customerID': ## removing the column CustomerID because it will not give importance in the predcition\n",
    "        le = LabelEncoder()  ## defining the label encoder\n",
    "        data[col] = le.fit_transform(data[col])  ## tranbsform the columns to encode all categorical columns\n",
    "        label_encoders[col] = le  # Save the encoder for deployment\n",
    "\n",
    "## Save the label encoders\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "## Separate features and target\n",
    "features = data.drop(['Churn', 'customerID'], axis=1).values  ## drop the customerID column\n",
    "target = data['Churn'].values\n",
    "print(\"Shape of Features in Raw Data : ---\",features.shape)\n",
    "print(\"Shape of Targets in Raw Data : ---\",target.shape)\n",
    "\n",
    "##  Apply SMOTE for oversampling the minority class\n",
    "## Synthetic Minority Over-sampling Technique\n",
    "smote = SMOTE(random_state=42)\n",
    "features_resampled, target_resampled = smote.fit_resample(features, target)\n",
    "print(\"Shape of Features after doing the SMOTE Technique : ---\",features_resampled.shape)\n",
    "print(\"Shape of Targets after doing the SMOTE Technique : ---\",target_resampled.shape)\n",
    "## Save the SMOTE object\n",
    "joblib.dump(smote, 'smote.pkl')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92cb0f8-41fb-4cd8-baa4-642e9469658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the hyperparameters\n",
    "## Define the hyperparameter optimization function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters\n",
    "    params = {\n",
    "        'n_d': trial.suggest_int('n_d', 8, 64),\n",
    "        'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "        'lambda_sparse': trial.suggest_float('lambda_sparse', 0.0001, 0.01),\n",
    "        'optimizer_params': {'lr': trial.suggest_float('lr', 1e-4, 1e-2)},\n",
    "        'mask_type': trial.suggest_categorical('mask_type', ['entmax', 'sparsemax']),\n",
    "        'n_shared': trial.suggest_int('n_shared', 1, 3),\n",
    "        'n_independent': trial.suggest_int('n_independent', 1, 3)\n",
    "    }\n",
    "    \n",
    "    # Initialize TabNetClassifier with the parameters\n",
    "    tabnet_clf = TabNetClassifier(\n",
    "        n_d=params['n_d'],\n",
    "        n_a=params['n_a'],\n",
    "        n_steps=params['n_steps'],\n",
    "        gamma=params['gamma'],\n",
    "        lambda_sparse=params['lambda_sparse'],\n",
    "        optimizer_params=params['optimizer_params'],\n",
    "        mask_type=params['mask_type'],\n",
    "        n_shared=params['n_shared'],\n",
    "        n_independent=params['n_independent']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    tabnet_clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=256,\n",
    "        virtual_batch_size=128\n",
    "    )\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = tabnet_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae1b781b-4d7b-47c6-ba40-697acb7ba36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-09 14:27:56,261] A new study created in memory with name: no-name-1ca64b1c-1e94-4ca8-a04b-759a4495ab9e\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.85121 | val_0_accuracy: 0.5343  |  0:00:03s\n",
      "epoch 1  | loss: 0.62698 | val_0_accuracy: 0.64976 |  0:00:10s\n",
      "epoch 2  | loss: 0.57145 | val_0_accuracy: 0.72174 |  0:00:16s\n",
      "epoch 3  | loss: 0.55581 | val_0_accuracy: 0.7343  |  0:00:27s\n",
      "epoch 4  | loss: 0.54609 | val_0_accuracy: 0.72899 |  0:00:37s\n",
      "epoch 5  | loss: 0.51952 | val_0_accuracy: 0.77005 |  0:00:43s\n",
      "epoch 6  | loss: 0.52895 | val_0_accuracy: 0.78792 |  0:00:50s\n",
      "epoch 7  | loss: 0.51045 | val_0_accuracy: 0.7686  |  0:00:56s\n",
      "epoch 8  | loss: 0.5068  | val_0_accuracy: 0.75845 |  0:01:02s\n",
      "epoch 9  | loss: 0.51035 | val_0_accuracy: 0.76329 |  0:01:08s\n",
      "epoch 10 | loss: 0.50876 | val_0_accuracy: 0.77488 |  0:01:13s\n",
      "epoch 11 | loss: 0.50459 | val_0_accuracy: 0.77536 |  0:01:19s\n",
      "epoch 12 | loss: 0.51326 | val_0_accuracy: 0.75266 |  0:01:25s\n",
      "epoch 13 | loss: 0.50742 | val_0_accuracy: 0.75797 |  0:01:31s\n",
      "epoch 14 | loss: 0.50134 | val_0_accuracy: 0.77923 |  0:01:36s\n",
      "epoch 15 | loss: 0.48908 | val_0_accuracy: 0.78406 |  0:01:42s\n",
      "epoch 16 | loss: 0.49702 | val_0_accuracy: 0.79179 |  0:01:48s\n",
      "epoch 17 | loss: 0.47434 | val_0_accuracy: 0.79082 |  0:01:54s\n",
      "epoch 18 | loss: 0.48831 | val_0_accuracy: 0.79662 |  0:02:00s\n",
      "epoch 19 | loss: 0.48955 | val_0_accuracy: 0.78116 |  0:02:06s\n",
      "epoch 20 | loss: 0.47551 | val_0_accuracy: 0.78647 |  0:02:13s\n",
      "epoch 21 | loss: 0.46814 | val_0_accuracy: 0.79758 |  0:02:18s\n",
      "epoch 22 | loss: 0.47037 | val_0_accuracy: 0.79469 |  0:02:24s\n",
      "epoch 23 | loss: 0.47069 | val_0_accuracy: 0.78019 |  0:02:29s\n",
      "epoch 24 | loss: 0.4673  | val_0_accuracy: 0.78889 |  0:02:35s\n",
      "epoch 25 | loss: 0.46626 | val_0_accuracy: 0.77923 |  0:02:41s\n",
      "epoch 26 | loss: 0.4636  | val_0_accuracy: 0.7715  |  0:02:47s\n",
      "epoch 27 | loss: 0.47491 | val_0_accuracy: 0.78551 |  0:02:54s\n",
      "epoch 28 | loss: 0.46211 | val_0_accuracy: 0.79324 |  0:03:00s\n",
      "epoch 29 | loss: 0.45578 | val_0_accuracy: 0.79034 |  0:03:06s\n",
      "epoch 30 | loss: 0.46556 | val_0_accuracy: 0.78696 |  0:03:12s\n",
      "epoch 31 | loss: 0.46218 | val_0_accuracy: 0.79372 |  0:03:19s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_accuracy = 0.79758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-09-09 14:31:19,956] Trial 0 finished with value: 0.797584541062802 and parameters: {'n_d': 23, 'n_a': 29, 'n_steps': 8, 'gamma': 1.9889182765540496, 'lambda_sparse': 0.004552405345846334, 'lr': 0.0043902484776940495, 'mask_type': 'entmax', 'n_shared': 2, 'n_independent': 3}. Best is trial 0 with value: 0.797584541062802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_d': 23, 'n_a': 29, 'n_steps': 8, 'gamma': 1.9889182765540496, 'lambda_sparse': 0.004552405345846334, 'lr': 0.0043902484776940495, 'mask_type': 'entmax', 'n_shared': 2, 'n_independent': 3}\n"
     ]
    }
   ],
   "source": [
    "## Run the hyperparameter optimization\n",
    "## this step is to get the best hyperparameters from the defined hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "445ccfe8-4cd5-4ce2-a1d3-afeebcb98f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.99832 |  0:00:03s\n",
      "epoch 1  | loss: 0.70475 |  0:00:07s\n",
      "epoch 2  | loss: 0.64388 |  0:00:12s\n",
      "epoch 3  | loss: 0.59762 |  0:00:16s\n",
      "epoch 4  | loss: 0.58686 |  0:00:19s\n",
      "epoch 5  | loss: 0.56716 |  0:00:22s\n",
      "epoch 6  | loss: 0.54784 |  0:00:25s\n",
      "epoch 7  | loss: 0.53829 |  0:00:29s\n",
      "epoch 8  | loss: 0.5365  |  0:00:32s\n",
      "epoch 9  | loss: 0.53782 |  0:00:36s\n",
      "epoch 10 | loss: 0.52871 |  0:00:39s\n",
      "epoch 11 | loss: 0.53489 |  0:00:42s\n",
      "epoch 12 | loss: 0.52635 |  0:00:45s\n",
      "epoch 13 | loss: 0.52194 |  0:00:48s\n",
      "epoch 14 | loss: 0.52556 |  0:00:51s\n",
      "epoch 15 | loss: 0.51255 |  0:00:54s\n",
      "epoch 16 | loss: 0.5018  |  0:00:56s\n",
      "epoch 17 | loss: 0.50491 |  0:01:00s\n",
      "epoch 18 | loss: 0.49649 |  0:01:03s\n",
      "epoch 19 | loss: 0.50394 |  0:01:06s\n",
      "epoch 20 | loss: 0.4932  |  0:01:08s\n",
      "epoch 21 | loss: 0.4793  |  0:01:11s\n",
      "epoch 22 | loss: 0.48236 |  0:01:13s\n",
      "epoch 23 | loss: 0.48775 |  0:01:17s\n",
      "epoch 24 | loss: 0.4871  |  0:01:19s\n",
      "epoch 25 | loss: 0.49325 |  0:01:22s\n",
      "epoch 26 | loss: 0.49116 |  0:01:27s\n",
      "epoch 27 | loss: 0.49392 |  0:01:30s\n",
      "epoch 28 | loss: 0.48445 |  0:01:33s\n",
      "epoch 29 | loss: 0.49019 |  0:01:36s\n",
      "epoch 30 | loss: 0.48514 |  0:01:39s\n",
      "epoch 31 | loss: 0.48651 |  0:01:42s\n",
      "epoch 32 | loss: 0.49604 |  0:01:46s\n",
      "epoch 33 | loss: 0.48455 |  0:01:51s\n",
      "epoch 34 | loss: 0.48466 |  0:01:54s\n",
      "epoch 35 | loss: 0.47998 |  0:01:57s\n",
      "epoch 36 | loss: 0.48608 |  0:02:00s\n",
      "epoch 37 | loss: 0.47433 |  0:02:04s\n",
      "epoch 38 | loss: 0.47675 |  0:02:08s\n",
      "epoch 39 | loss: 0.4702  |  0:02:11s\n",
      "epoch 40 | loss: 0.46735 |  0:02:14s\n",
      "epoch 41 | loss: 0.46927 |  0:02:17s\n",
      "epoch 42 | loss: 0.46405 |  0:02:19s\n",
      "epoch 43 | loss: 0.46576 |  0:02:22s\n",
      "epoch 44 | loss: 0.46994 |  0:02:25s\n",
      "epoch 45 | loss: 0.48175 |  0:02:27s\n",
      "epoch 46 | loss: 0.4746  |  0:02:30s\n",
      "epoch 47 | loss: 0.47497 |  0:02:33s\n",
      "epoch 48 | loss: 0.47015 |  0:02:36s\n",
      "epoch 49 | loss: 0.48323 |  0:02:39s\n",
      "epoch 50 | loss: 0.47465 |  0:02:41s\n",
      "epoch 51 | loss: 0.4682  |  0:02:44s\n",
      "epoch 52 | loss: 0.47748 |  0:02:48s\n",
      "epoch 53 | loss: 0.46859 |  0:02:51s\n",
      "epoch 54 | loss: 0.45973 |  0:02:56s\n",
      "epoch 55 | loss: 0.45415 |  0:02:59s\n",
      "epoch 56 | loss: 0.45761 |  0:03:02s\n",
      "epoch 57 | loss: 0.45583 |  0:03:06s\n",
      "epoch 58 | loss: 0.44692 |  0:03:09s\n",
      "epoch 59 | loss: 0.4577  |  0:03:12s\n",
      "epoch 60 | loss: 0.45459 |  0:03:15s\n",
      "epoch 61 | loss: 0.46888 |  0:03:18s\n",
      "epoch 62 | loss: 0.45936 |  0:03:20s\n",
      "epoch 63 | loss: 0.48543 |  0:03:23s\n",
      "epoch 64 | loss: 0.47652 |  0:03:26s\n",
      "epoch 65 | loss: 0.46627 |  0:03:29s\n",
      "epoch 66 | loss: 0.45668 |  0:03:32s\n",
      "epoch 67 | loss: 0.45196 |  0:03:35s\n",
      "epoch 68 | loss: 0.44881 |  0:03:38s\n",
      "epoch 69 | loss: 0.45142 |  0:03:42s\n",
      "epoch 70 | loss: 0.44951 |  0:03:44s\n",
      "epoch 71 | loss: 0.4405  |  0:03:47s\n",
      "epoch 72 | loss: 0.44645 |  0:03:50s\n",
      "epoch 73 | loss: 0.44759 |  0:03:54s\n",
      "epoch 74 | loss: 0.44833 |  0:03:56s\n",
      "epoch 75 | loss: 0.45388 |  0:04:00s\n",
      "epoch 76 | loss: 0.45593 |  0:04:03s\n",
      "epoch 77 | loss: 0.44649 |  0:04:06s\n",
      "epoch 78 | loss: 0.446   |  0:04:08s\n",
      "epoch 79 | loss: 0.44825 |  0:04:10s\n",
      "epoch 80 | loss: 0.4412  |  0:04:13s\n",
      "epoch 81 | loss: 0.44039 |  0:04:15s\n",
      "epoch 82 | loss: 0.44232 |  0:04:17s\n",
      "epoch 83 | loss: 0.44389 |  0:04:19s\n",
      "epoch 84 | loss: 0.4423  |  0:04:22s\n",
      "epoch 85 | loss: 0.44867 |  0:04:24s\n",
      "epoch 86 | loss: 0.44318 |  0:04:27s\n",
      "epoch 87 | loss: 0.43848 |  0:04:29s\n",
      "epoch 88 | loss: 0.44275 |  0:04:31s\n",
      "epoch 89 | loss: 0.44028 |  0:04:33s\n",
      "epoch 90 | loss: 0.4491  |  0:04:36s\n",
      "epoch 91 | loss: 0.44849 |  0:04:39s\n",
      "epoch 92 | loss: 0.45696 |  0:04:41s\n",
      "epoch 93 | loss: 0.45201 |  0:04:43s\n",
      "epoch 94 | loss: 0.44319 |  0:04:45s\n",
      "epoch 95 | loss: 0.44486 |  0:04:48s\n",
      "epoch 96 | loss: 0.44867 |  0:04:51s\n",
      "epoch 97 | loss: 0.45511 |  0:04:55s\n",
      "epoch 98 | loss: 0.4547  |  0:04:57s\n",
      "epoch 99 | loss: 0.46164 |  0:05:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.04078 |  0:00:02s\n",
      "epoch 1  | loss: 0.75535 |  0:00:05s\n",
      "epoch 2  | loss: 0.66623 |  0:00:08s\n",
      "epoch 3  | loss: 0.60226 |  0:00:10s\n",
      "epoch 4  | loss: 0.56409 |  0:00:13s\n",
      "epoch 5  | loss: 0.55288 |  0:00:16s\n",
      "epoch 6  | loss: 0.53676 |  0:00:19s\n",
      "epoch 7  | loss: 0.53344 |  0:00:21s\n",
      "epoch 8  | loss: 0.52739 |  0:00:22s\n",
      "epoch 9  | loss: 0.5367  |  0:00:24s\n",
      "epoch 10 | loss: 0.53051 |  0:00:27s\n",
      "epoch 11 | loss: 0.52324 |  0:00:29s\n",
      "epoch 12 | loss: 0.52898 |  0:00:31s\n",
      "epoch 13 | loss: 0.53066 |  0:00:33s\n",
      "epoch 14 | loss: 0.5571  |  0:00:36s\n",
      "epoch 15 | loss: 0.52522 |  0:00:38s\n",
      "epoch 16 | loss: 0.50748 |  0:00:40s\n",
      "epoch 17 | loss: 0.49906 |  0:00:42s\n",
      "epoch 18 | loss: 0.49279 |  0:00:45s\n",
      "epoch 19 | loss: 0.49732 |  0:00:47s\n",
      "epoch 20 | loss: 0.49256 |  0:00:50s\n",
      "epoch 21 | loss: 0.49292 |  0:00:52s\n",
      "epoch 22 | loss: 0.48799 |  0:00:55s\n",
      "epoch 23 | loss: 0.48576 |  0:00:57s\n",
      "epoch 24 | loss: 0.48565 |  0:00:59s\n",
      "epoch 25 | loss: 0.47464 |  0:01:01s\n",
      "epoch 26 | loss: 0.49586 |  0:01:03s\n",
      "epoch 27 | loss: 0.48591 |  0:01:05s\n",
      "epoch 28 | loss: 0.4761  |  0:01:08s\n",
      "epoch 29 | loss: 0.47802 |  0:01:10s\n",
      "epoch 30 | loss: 0.47994 |  0:01:14s\n",
      "epoch 31 | loss: 0.50735 |  0:01:16s\n",
      "epoch 32 | loss: 0.49374 |  0:01:18s\n",
      "epoch 33 | loss: 0.49603 |  0:01:21s\n",
      "epoch 34 | loss: 0.47859 |  0:01:23s\n",
      "epoch 35 | loss: 0.47862 |  0:01:25s\n",
      "epoch 36 | loss: 0.46991 |  0:01:28s\n",
      "epoch 37 | loss: 0.46917 |  0:01:30s\n",
      "epoch 38 | loss: 0.47351 |  0:01:33s\n",
      "epoch 39 | loss: 0.4705  |  0:01:36s\n",
      "epoch 40 | loss: 0.46462 |  0:01:38s\n",
      "epoch 41 | loss: 0.46408 |  0:01:40s\n",
      "epoch 42 | loss: 0.46292 |  0:01:43s\n",
      "epoch 43 | loss: 0.46563 |  0:01:47s\n",
      "epoch 44 | loss: 0.462   |  0:01:53s\n",
      "epoch 45 | loss: 0.46251 |  0:01:58s\n",
      "epoch 46 | loss: 0.46343 |  0:02:03s\n",
      "epoch 47 | loss: 0.46818 |  0:02:09s\n",
      "epoch 48 | loss: 0.47697 |  0:02:12s\n",
      "epoch 49 | loss: 0.47269 |  0:02:17s\n",
      "epoch 50 | loss: 0.45294 |  0:02:20s\n",
      "epoch 51 | loss: 0.45895 |  0:02:23s\n",
      "epoch 52 | loss: 0.46736 |  0:02:25s\n",
      "epoch 53 | loss: 0.45569 |  0:02:28s\n",
      "epoch 54 | loss: 0.45474 |  0:02:30s\n",
      "epoch 55 | loss: 0.44871 |  0:02:33s\n",
      "epoch 56 | loss: 0.45409 |  0:02:35s\n",
      "epoch 57 | loss: 0.45276 |  0:02:38s\n",
      "epoch 58 | loss: 0.45012 |  0:02:41s\n",
      "epoch 59 | loss: 0.45513 |  0:02:43s\n",
      "epoch 60 | loss: 0.45176 |  0:02:45s\n",
      "epoch 61 | loss: 0.45241 |  0:02:48s\n",
      "epoch 62 | loss: 0.44497 |  0:02:51s\n",
      "epoch 63 | loss: 0.44659 |  0:02:53s\n",
      "epoch 64 | loss: 0.44277 |  0:02:55s\n",
      "epoch 65 | loss: 0.43748 |  0:02:57s\n",
      "epoch 66 | loss: 0.44687 |  0:03:00s\n",
      "epoch 67 | loss: 0.4447  |  0:03:02s\n",
      "epoch 68 | loss: 0.44581 |  0:03:04s\n",
      "epoch 69 | loss: 0.44541 |  0:03:08s\n",
      "epoch 70 | loss: 0.43908 |  0:03:10s\n",
      "epoch 71 | loss: 0.44212 |  0:03:13s\n",
      "epoch 72 | loss: 0.4484  |  0:03:15s\n",
      "epoch 73 | loss: 0.44906 |  0:03:18s\n",
      "epoch 74 | loss: 0.44543 |  0:03:21s\n",
      "epoch 75 | loss: 0.4457  |  0:03:24s\n",
      "epoch 76 | loss: 0.44846 |  0:03:28s\n",
      "epoch 77 | loss: 0.44637 |  0:03:31s\n",
      "epoch 78 | loss: 0.45756 |  0:03:35s\n",
      "epoch 79 | loss: 0.4589  |  0:03:39s\n",
      "epoch 80 | loss: 0.44676 |  0:03:42s\n",
      "epoch 81 | loss: 0.45752 |  0:03:45s\n",
      "epoch 82 | loss: 0.44961 |  0:03:49s\n",
      "epoch 83 | loss: 0.44725 |  0:03:51s\n",
      "epoch 84 | loss: 0.44569 |  0:03:54s\n",
      "epoch 85 | loss: 0.44302 |  0:03:58s\n",
      "epoch 86 | loss: 0.44386 |  0:04:01s\n",
      "epoch 87 | loss: 0.43707 |  0:04:05s\n",
      "epoch 88 | loss: 0.43586 |  0:04:09s\n",
      "epoch 89 | loss: 0.43668 |  0:04:12s\n",
      "epoch 90 | loss: 0.43996 |  0:04:14s\n",
      "epoch 91 | loss: 0.43272 |  0:04:17s\n",
      "epoch 92 | loss: 0.43466 |  0:04:19s\n",
      "epoch 93 | loss: 0.43213 |  0:04:21s\n",
      "epoch 94 | loss: 0.43555 |  0:04:24s\n",
      "epoch 95 | loss: 0.42888 |  0:04:27s\n",
      "epoch 96 | loss: 0.43232 |  0:04:29s\n",
      "epoch 97 | loss: 0.43319 |  0:04:31s\n",
      "epoch 98 | loss: 0.43896 |  0:04:34s\n",
      "epoch 99 | loss: 0.4324  |  0:04:36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.10099 |  0:00:03s\n",
      "epoch 1  | loss: 0.82752 |  0:00:05s\n",
      "epoch 2  | loss: 0.71027 |  0:00:08s\n",
      "epoch 3  | loss: 0.64346 |  0:00:11s\n",
      "epoch 4  | loss: 0.6221  |  0:00:13s\n",
      "epoch 5  | loss: 0.589   |  0:00:15s\n",
      "epoch 6  | loss: 0.56873 |  0:00:19s\n",
      "epoch 7  | loss: 0.56076 |  0:00:22s\n",
      "epoch 8  | loss: 0.53911 |  0:00:24s\n",
      "epoch 9  | loss: 0.5278  |  0:00:26s\n",
      "epoch 10 | loss: 0.50902 |  0:00:29s\n",
      "epoch 11 | loss: 0.50843 |  0:00:31s\n",
      "epoch 12 | loss: 0.50458 |  0:00:34s\n",
      "epoch 13 | loss: 0.50365 |  0:00:36s\n",
      "epoch 14 | loss: 0.50452 |  0:00:38s\n",
      "epoch 15 | loss: 0.51185 |  0:00:41s\n",
      "epoch 16 | loss: 0.51004 |  0:00:44s\n",
      "epoch 17 | loss: 0.49696 |  0:00:47s\n",
      "epoch 18 | loss: 0.49103 |  0:00:49s\n",
      "epoch 19 | loss: 0.49406 |  0:00:52s\n",
      "epoch 20 | loss: 0.49722 |  0:00:54s\n",
      "epoch 21 | loss: 0.48419 |  0:00:57s\n",
      "epoch 22 | loss: 0.47546 |  0:00:59s\n",
      "epoch 23 | loss: 0.47511 |  0:01:01s\n",
      "epoch 24 | loss: 0.47721 |  0:01:04s\n",
      "epoch 25 | loss: 0.4805  |  0:01:06s\n",
      "epoch 26 | loss: 0.48848 |  0:01:08s\n",
      "epoch 27 | loss: 0.47906 |  0:01:13s\n",
      "epoch 28 | loss: 0.48259 |  0:01:17s\n",
      "epoch 29 | loss: 0.48729 |  0:01:21s\n",
      "epoch 30 | loss: 0.49725 |  0:01:24s\n",
      "epoch 31 | loss: 0.49453 |  0:01:27s\n",
      "epoch 32 | loss: 0.48756 |  0:01:31s\n",
      "epoch 33 | loss: 0.48476 |  0:01:35s\n",
      "epoch 34 | loss: 0.4691  |  0:01:38s\n",
      "epoch 35 | loss: 0.47005 |  0:01:42s\n",
      "epoch 36 | loss: 0.47902 |  0:01:45s\n",
      "epoch 37 | loss: 0.47373 |  0:01:48s\n",
      "epoch 38 | loss: 0.47302 |  0:01:51s\n",
      "epoch 39 | loss: 0.48237 |  0:01:57s\n",
      "epoch 40 | loss: 0.48966 |  0:02:00s\n",
      "epoch 41 | loss: 0.48334 |  0:02:03s\n",
      "epoch 42 | loss: 0.48271 |  0:02:07s\n",
      "epoch 43 | loss: 0.49028 |  0:02:09s\n",
      "epoch 44 | loss: 0.48237 |  0:02:12s\n",
      "epoch 45 | loss: 0.47847 |  0:02:15s\n",
      "epoch 46 | loss: 0.4869  |  0:02:17s\n",
      "epoch 47 | loss: 0.48643 |  0:02:20s\n",
      "epoch 48 | loss: 0.48443 |  0:02:22s\n",
      "epoch 49 | loss: 0.48882 |  0:02:25s\n",
      "epoch 50 | loss: 0.48216 |  0:02:28s\n",
      "epoch 51 | loss: 0.48398 |  0:02:31s\n",
      "epoch 52 | loss: 0.48383 |  0:02:34s\n",
      "epoch 53 | loss: 0.47918 |  0:02:38s\n",
      "epoch 54 | loss: 0.48222 |  0:02:41s\n",
      "epoch 55 | loss: 0.48189 |  0:02:43s\n",
      "epoch 56 | loss: 0.48241 |  0:02:46s\n",
      "epoch 57 | loss: 0.48264 |  0:02:48s\n",
      "epoch 58 | loss: 0.47808 |  0:02:51s\n",
      "epoch 59 | loss: 0.47858 |  0:02:54s\n",
      "epoch 60 | loss: 0.48208 |  0:02:57s\n",
      "epoch 61 | loss: 0.48633 |  0:03:00s\n",
      "epoch 62 | loss: 0.48211 |  0:03:02s\n",
      "epoch 63 | loss: 0.46619 |  0:03:04s\n",
      "epoch 64 | loss: 0.47111 |  0:03:07s\n",
      "epoch 65 | loss: 0.46345 |  0:03:09s\n",
      "epoch 66 | loss: 0.46442 |  0:03:12s\n",
      "epoch 67 | loss: 0.45496 |  0:03:14s\n",
      "epoch 68 | loss: 0.46663 |  0:03:17s\n",
      "epoch 69 | loss: 0.46746 |  0:03:19s\n",
      "epoch 70 | loss: 0.46589 |  0:03:21s\n",
      "epoch 71 | loss: 0.46208 |  0:03:24s\n",
      "epoch 72 | loss: 0.46318 |  0:03:26s\n",
      "epoch 73 | loss: 0.46226 |  0:03:29s\n",
      "epoch 74 | loss: 0.45779 |  0:03:32s\n",
      "epoch 75 | loss: 0.45872 |  0:03:35s\n",
      "epoch 76 | loss: 0.45842 |  0:03:39s\n",
      "epoch 77 | loss: 0.46339 |  0:03:42s\n",
      "epoch 78 | loss: 0.47332 |  0:03:45s\n",
      "epoch 79 | loss: 0.47127 |  0:03:48s\n",
      "epoch 80 | loss: 0.47788 |  0:03:51s\n",
      "epoch 81 | loss: 0.47045 |  0:03:54s\n",
      "epoch 82 | loss: 0.46015 |  0:03:56s\n",
      "epoch 83 | loss: 0.45003 |  0:04:00s\n",
      "epoch 84 | loss: 0.46382 |  0:04:04s\n",
      "epoch 85 | loss: 0.45497 |  0:04:06s\n",
      "epoch 86 | loss: 0.45795 |  0:04:09s\n",
      "epoch 87 | loss: 0.46163 |  0:04:11s\n",
      "epoch 88 | loss: 0.46477 |  0:04:13s\n",
      "epoch 89 | loss: 0.45589 |  0:04:16s\n",
      "epoch 90 | loss: 0.45474 |  0:04:18s\n",
      "epoch 91 | loss: 0.45036 |  0:04:21s\n",
      "epoch 92 | loss: 0.44787 |  0:04:24s\n",
      "epoch 93 | loss: 0.45775 |  0:04:28s\n",
      "epoch 94 | loss: 0.44975 |  0:04:31s\n",
      "epoch 95 | loss: 0.45509 |  0:04:34s\n",
      "epoch 96 | loss: 0.44837 |  0:04:36s\n",
      "epoch 97 | loss: 0.45948 |  0:04:39s\n",
      "epoch 98 | loss: 0.48087 |  0:04:41s\n",
      "epoch 99 | loss: 0.48865 |  0:04:44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.01198 |  0:00:03s\n",
      "epoch 1  | loss: 0.73262 |  0:00:06s\n",
      "epoch 2  | loss: 0.64642 |  0:00:08s\n",
      "epoch 3  | loss: 0.59773 |  0:00:11s\n",
      "epoch 4  | loss: 0.56888 |  0:00:13s\n",
      "epoch 5  | loss: 0.56854 |  0:00:15s\n",
      "epoch 6  | loss: 0.54994 |  0:00:18s\n",
      "epoch 7  | loss: 0.53959 |  0:00:21s\n",
      "epoch 8  | loss: 0.53687 |  0:00:24s\n",
      "epoch 9  | loss: 0.52077 |  0:00:29s\n",
      "epoch 10 | loss: 0.50797 |  0:00:32s\n",
      "epoch 11 | loss: 0.50689 |  0:00:35s\n",
      "epoch 12 | loss: 0.50246 |  0:00:39s\n",
      "epoch 13 | loss: 0.50133 |  0:00:42s\n",
      "epoch 14 | loss: 0.50627 |  0:00:44s\n",
      "epoch 15 | loss: 0.50131 |  0:00:47s\n",
      "epoch 16 | loss: 0.5118  |  0:00:50s\n",
      "epoch 17 | loss: 0.51375 |  0:00:52s\n",
      "epoch 18 | loss: 0.52618 |  0:00:55s\n",
      "epoch 19 | loss: 0.51036 |  0:00:58s\n",
      "epoch 20 | loss: 0.49367 |  0:01:01s\n",
      "epoch 21 | loss: 0.49533 |  0:01:04s\n",
      "epoch 22 | loss: 0.49412 |  0:01:06s\n",
      "epoch 23 | loss: 0.50875 |  0:01:09s\n",
      "epoch 24 | loss: 0.51089 |  0:01:11s\n",
      "epoch 25 | loss: 0.49139 |  0:01:14s\n",
      "epoch 26 | loss: 0.49607 |  0:01:17s\n",
      "epoch 27 | loss: 0.49475 |  0:01:20s\n",
      "epoch 28 | loss: 0.49068 |  0:01:22s\n",
      "epoch 29 | loss: 0.49459 |  0:01:24s\n",
      "epoch 30 | loss: 0.49442 |  0:01:27s\n",
      "epoch 31 | loss: 0.48638 |  0:01:31s\n",
      "epoch 32 | loss: 0.49359 |  0:01:33s\n",
      "epoch 33 | loss: 0.49506 |  0:01:36s\n",
      "epoch 34 | loss: 0.48432 |  0:01:39s\n",
      "epoch 35 | loss: 0.495   |  0:01:42s\n",
      "epoch 36 | loss: 0.50046 |  0:01:46s\n",
      "epoch 37 | loss: 0.49096 |  0:01:50s\n",
      "epoch 38 | loss: 0.48488 |  0:01:53s\n",
      "epoch 39 | loss: 0.48266 |  0:01:57s\n",
      "epoch 40 | loss: 0.498   |  0:02:02s\n",
      "epoch 41 | loss: 0.49047 |  0:02:07s\n",
      "epoch 42 | loss: 0.47364 |  0:02:10s\n",
      "epoch 43 | loss: 0.48012 |  0:02:13s\n",
      "epoch 44 | loss: 0.46836 |  0:02:16s\n",
      "epoch 45 | loss: 0.4893  |  0:02:20s\n",
      "epoch 46 | loss: 0.49274 |  0:02:24s\n",
      "epoch 47 | loss: 0.49851 |  0:02:28s\n",
      "epoch 48 | loss: 0.49106 |  0:02:31s\n",
      "epoch 49 | loss: 0.48934 |  0:02:35s\n",
      "epoch 50 | loss: 0.47386 |  0:02:37s\n",
      "epoch 51 | loss: 0.47104 |  0:02:40s\n",
      "epoch 52 | loss: 0.46826 |  0:02:42s\n",
      "epoch 53 | loss: 0.47341 |  0:02:45s\n",
      "epoch 54 | loss: 0.47578 |  0:02:47s\n",
      "epoch 55 | loss: 0.47539 |  0:02:50s\n",
      "epoch 56 | loss: 0.4709  |  0:02:52s\n",
      "epoch 57 | loss: 0.47527 |  0:02:54s\n",
      "epoch 58 | loss: 0.46965 |  0:02:57s\n",
      "epoch 59 | loss: 0.47135 |  0:02:59s\n",
      "epoch 60 | loss: 0.47084 |  0:03:01s\n",
      "epoch 61 | loss: 0.47551 |  0:03:05s\n",
      "epoch 62 | loss: 0.46613 |  0:03:09s\n",
      "epoch 63 | loss: 0.46546 |  0:03:11s\n",
      "epoch 64 | loss: 0.45826 |  0:03:16s\n",
      "epoch 65 | loss: 0.46378 |  0:03:19s\n",
      "epoch 66 | loss: 0.46416 |  0:03:23s\n",
      "epoch 67 | loss: 0.46393 |  0:03:25s\n",
      "epoch 68 | loss: 0.46472 |  0:03:28s\n",
      "epoch 69 | loss: 0.48226 |  0:03:31s\n",
      "epoch 70 | loss: 0.46985 |  0:03:36s\n",
      "epoch 71 | loss: 0.45712 |  0:03:38s\n",
      "epoch 72 | loss: 0.45556 |  0:03:41s\n",
      "epoch 73 | loss: 0.45303 |  0:03:44s\n",
      "epoch 74 | loss: 0.44833 |  0:03:46s\n",
      "epoch 75 | loss: 0.44932 |  0:03:49s\n",
      "epoch 76 | loss: 0.45232 |  0:03:52s\n",
      "epoch 77 | loss: 0.45951 |  0:03:55s\n",
      "epoch 78 | loss: 0.45891 |  0:03:58s\n",
      "epoch 79 | loss: 0.45172 |  0:04:01s\n",
      "epoch 80 | loss: 0.44388 |  0:04:04s\n",
      "epoch 81 | loss: 0.44479 |  0:04:07s\n",
      "epoch 82 | loss: 0.45037 |  0:04:09s\n",
      "epoch 83 | loss: 0.44905 |  0:04:11s\n",
      "epoch 84 | loss: 0.44839 |  0:04:14s\n",
      "epoch 85 | loss: 0.44093 |  0:04:16s\n",
      "epoch 86 | loss: 0.44799 |  0:04:18s\n",
      "epoch 87 | loss: 0.44689 |  0:04:21s\n",
      "epoch 88 | loss: 0.43962 |  0:04:24s\n",
      "epoch 89 | loss: 0.446   |  0:04:26s\n",
      "epoch 90 | loss: 0.44091 |  0:04:29s\n",
      "epoch 91 | loss: 0.43853 |  0:04:32s\n",
      "epoch 92 | loss: 0.4359  |  0:04:35s\n",
      "epoch 93 | loss: 0.43689 |  0:04:38s\n",
      "epoch 94 | loss: 0.43898 |  0:04:40s\n",
      "epoch 95 | loss: 0.43876 |  0:04:43s\n",
      "epoch 96 | loss: 0.43231 |  0:04:46s\n",
      "epoch 97 | loss: 0.43577 |  0:04:49s\n",
      "epoch 98 | loss: 0.43885 |  0:04:53s\n",
      "epoch 99 | loss: 0.44289 |  0:04:56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.05808 |  0:00:04s\n",
      "epoch 1  | loss: 0.76057 |  0:00:07s\n",
      "epoch 2  | loss: 0.63957 |  0:00:10s\n",
      "epoch 3  | loss: 0.59593 |  0:00:13s\n",
      "epoch 4  | loss: 0.56737 |  0:00:17s\n",
      "epoch 5  | loss: 0.55201 |  0:00:21s\n",
      "epoch 6  | loss: 0.54401 |  0:00:25s\n",
      "epoch 7  | loss: 0.53671 |  0:00:29s\n",
      "epoch 8  | loss: 0.5301  |  0:00:33s\n",
      "epoch 9  | loss: 0.53862 |  0:00:37s\n",
      "epoch 10 | loss: 0.53064 |  0:00:40s\n",
      "epoch 11 | loss: 0.51804 |  0:00:43s\n",
      "epoch 12 | loss: 0.4992  |  0:00:46s\n",
      "epoch 13 | loss: 0.50137 |  0:00:49s\n",
      "epoch 14 | loss: 0.50721 |  0:00:52s\n",
      "epoch 15 | loss: 0.50717 |  0:00:55s\n",
      "epoch 16 | loss: 0.51596 |  0:00:58s\n",
      "epoch 17 | loss: 0.49525 |  0:01:01s\n",
      "epoch 18 | loss: 0.51578 |  0:01:03s\n",
      "epoch 19 | loss: 0.5056  |  0:01:06s\n",
      "epoch 20 | loss: 0.50621 |  0:01:08s\n",
      "epoch 21 | loss: 0.54212 |  0:01:11s\n",
      "epoch 22 | loss: 0.49725 |  0:01:14s\n",
      "epoch 23 | loss: 0.49958 |  0:01:17s\n",
      "epoch 24 | loss: 0.49579 |  0:01:20s\n",
      "epoch 25 | loss: 0.49383 |  0:01:22s\n",
      "epoch 26 | loss: 0.49778 |  0:01:26s\n",
      "epoch 27 | loss: 0.49199 |  0:01:29s\n",
      "epoch 28 | loss: 0.48294 |  0:01:32s\n",
      "epoch 29 | loss: 0.4869  |  0:01:35s\n",
      "epoch 30 | loss: 0.47714 |  0:01:38s\n",
      "epoch 31 | loss: 0.48624 |  0:01:41s\n",
      "epoch 32 | loss: 0.47473 |  0:01:45s\n",
      "epoch 33 | loss: 0.47765 |  0:01:48s\n",
      "epoch 34 | loss: 0.48434 |  0:01:52s\n",
      "epoch 35 | loss: 0.48934 |  0:01:57s\n",
      "epoch 36 | loss: 0.47647 |  0:02:00s\n",
      "epoch 37 | loss: 0.47385 |  0:02:02s\n",
      "epoch 38 | loss: 0.48182 |  0:02:07s\n",
      "epoch 39 | loss: 0.4745  |  0:02:11s\n",
      "epoch 40 | loss: 0.48315 |  0:02:15s\n",
      "epoch 41 | loss: 0.48132 |  0:02:20s\n",
      "epoch 42 | loss: 0.48625 |  0:02:26s\n",
      "epoch 43 | loss: 0.47075 |  0:02:31s\n",
      "epoch 44 | loss: 0.46871 |  0:02:36s\n",
      "epoch 45 | loss: 0.4699  |  0:02:40s\n",
      "epoch 46 | loss: 0.47063 |  0:02:43s\n",
      "epoch 47 | loss: 0.47075 |  0:02:45s\n",
      "epoch 48 | loss: 0.4686  |  0:02:49s\n",
      "epoch 49 | loss: 0.46711 |  0:02:51s\n",
      "epoch 50 | loss: 0.46262 |  0:02:55s\n",
      "epoch 51 | loss: 0.46807 |  0:02:58s\n",
      "epoch 52 | loss: 0.47515 |  0:03:02s\n",
      "epoch 53 | loss: 0.47456 |  0:03:07s\n",
      "epoch 54 | loss: 0.46618 |  0:03:14s\n",
      "epoch 55 | loss: 0.47053 |  0:03:18s\n",
      "epoch 56 | loss: 0.46878 |  0:03:20s\n",
      "epoch 57 | loss: 0.47031 |  0:03:22s\n",
      "epoch 58 | loss: 0.46988 |  0:03:25s\n",
      "epoch 59 | loss: 0.4658  |  0:03:29s\n",
      "epoch 60 | loss: 0.46909 |  0:03:32s\n",
      "epoch 61 | loss: 0.47059 |  0:03:34s\n",
      "epoch 62 | loss: 0.48071 |  0:03:38s\n",
      "epoch 63 | loss: 0.46587 |  0:03:42s\n",
      "epoch 64 | loss: 0.48861 |  0:03:45s\n",
      "epoch 65 | loss: 0.4827  |  0:03:48s\n",
      "epoch 66 | loss: 0.4909  |  0:03:52s\n",
      "epoch 67 | loss: 0.4796  |  0:03:54s\n",
      "epoch 68 | loss: 0.47872 |  0:03:57s\n",
      "epoch 69 | loss: 0.47839 |  0:03:59s\n",
      "epoch 70 | loss: 0.47235 |  0:04:02s\n",
      "epoch 71 | loss: 0.46642 |  0:04:04s\n",
      "epoch 72 | loss: 0.46777 |  0:04:10s\n",
      "epoch 73 | loss: 0.46757 |  0:04:13s\n",
      "epoch 74 | loss: 0.46667 |  0:04:15s\n",
      "epoch 75 | loss: 0.45875 |  0:04:18s\n",
      "epoch 76 | loss: 0.4641  |  0:04:22s\n",
      "epoch 77 | loss: 0.45853 |  0:04:24s\n",
      "epoch 78 | loss: 0.45657 |  0:04:27s\n",
      "epoch 79 | loss: 0.457   |  0:04:29s\n",
      "epoch 80 | loss: 0.45439 |  0:04:32s\n",
      "epoch 81 | loss: 0.45533 |  0:04:35s\n",
      "epoch 82 | loss: 0.45396 |  0:04:38s\n",
      "epoch 83 | loss: 0.45773 |  0:04:42s\n",
      "epoch 84 | loss: 0.45174 |  0:04:45s\n",
      "epoch 85 | loss: 0.44967 |  0:04:48s\n",
      "epoch 86 | loss: 0.44056 |  0:04:52s\n",
      "epoch 87 | loss: 0.45212 |  0:04:54s\n",
      "epoch 88 | loss: 0.44095 |  0:04:57s\n",
      "epoch 89 | loss: 0.44586 |  0:05:00s\n",
      "epoch 90 | loss: 0.44398 |  0:05:04s\n",
      "epoch 91 | loss: 0.45008 |  0:05:07s\n",
      "epoch 92 | loss: 0.44829 |  0:05:10s\n",
      "epoch 93 | loss: 0.44458 |  0:05:14s\n",
      "epoch 94 | loss: 0.45432 |  0:05:17s\n",
      "epoch 95 | loss: 0.45649 |  0:05:20s\n",
      "epoch 96 | loss: 0.45754 |  0:05:23s\n",
      "epoch 97 | loss: 0.45291 |  0:05:26s\n",
      "epoch 98 | loss: 0.44645 |  0:05:29s\n",
      "epoch 99 | loss: 0.4394  |  0:05:32s\n",
      "Cross-Validated Accuracy: 0.7864321608040201\n",
      "Cross-Validated Precision: 0.76\n",
      "Cross-Validated Recall: 0.83\n",
      "Cross-Validated F1 Score: 0.80\n",
      "epoch 0  | loss: 0.85121 | val_0_accuracy: 0.5343  |  0:00:06s\n",
      "epoch 1  | loss: 0.62698 | val_0_accuracy: 0.64976 |  0:00:13s\n",
      "epoch 2  | loss: 0.57145 | val_0_accuracy: 0.72174 |  0:00:19s\n",
      "epoch 3  | loss: 0.55581 | val_0_accuracy: 0.7343  |  0:00:26s\n",
      "epoch 4  | loss: 0.54609 | val_0_accuracy: 0.72899 |  0:00:32s\n",
      "epoch 5  | loss: 0.51952 | val_0_accuracy: 0.77005 |  0:00:38s\n",
      "epoch 6  | loss: 0.52895 | val_0_accuracy: 0.78792 |  0:00:44s\n",
      "epoch 7  | loss: 0.51045 | val_0_accuracy: 0.7686  |  0:00:51s\n",
      "epoch 8  | loss: 0.5068  | val_0_accuracy: 0.75845 |  0:00:58s\n",
      "epoch 9  | loss: 0.51035 | val_0_accuracy: 0.76329 |  0:01:03s\n",
      "epoch 10 | loss: 0.50876 | val_0_accuracy: 0.77488 |  0:01:08s\n",
      "epoch 11 | loss: 0.50459 | val_0_accuracy: 0.77536 |  0:01:14s\n",
      "epoch 12 | loss: 0.51326 | val_0_accuracy: 0.75266 |  0:01:20s\n",
      "epoch 13 | loss: 0.50742 | val_0_accuracy: 0.75797 |  0:01:28s\n",
      "epoch 14 | loss: 0.50134 | val_0_accuracy: 0.77923 |  0:01:40s\n",
      "epoch 15 | loss: 0.48908 | val_0_accuracy: 0.78406 |  0:01:48s\n",
      "epoch 16 | loss: 0.49702 | val_0_accuracy: 0.79179 |  0:01:53s\n",
      "epoch 17 | loss: 0.47434 | val_0_accuracy: 0.79082 |  0:02:00s\n",
      "epoch 18 | loss: 0.48831 | val_0_accuracy: 0.79662 |  0:02:09s\n",
      "epoch 19 | loss: 0.48955 | val_0_accuracy: 0.78116 |  0:02:15s\n",
      "epoch 20 | loss: 0.47551 | val_0_accuracy: 0.78647 |  0:02:21s\n",
      "epoch 21 | loss: 0.46814 | val_0_accuracy: 0.79758 |  0:02:26s\n",
      "epoch 22 | loss: 0.47037 | val_0_accuracy: 0.79469 |  0:02:32s\n",
      "epoch 23 | loss: 0.47069 | val_0_accuracy: 0.78019 |  0:02:38s\n",
      "epoch 24 | loss: 0.4673  | val_0_accuracy: 0.78889 |  0:02:44s\n",
      "epoch 25 | loss: 0.46626 | val_0_accuracy: 0.77923 |  0:02:53s\n",
      "epoch 26 | loss: 0.4636  | val_0_accuracy: 0.7715  |  0:03:01s\n",
      "epoch 27 | loss: 0.47491 | val_0_accuracy: 0.78551 |  0:03:08s\n",
      "epoch 28 | loss: 0.46211 | val_0_accuracy: 0.79324 |  0:03:16s\n",
      "epoch 29 | loss: 0.45578 | val_0_accuracy: 0.79034 |  0:03:24s\n",
      "epoch 30 | loss: 0.46556 | val_0_accuracy: 0.78696 |  0:03:31s\n",
      "epoch 31 | loss: 0.46218 | val_0_accuracy: 0.79372 |  0:03:37s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_accuracy = 0.79758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faiz Salam\\AnacondaNavigator\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Optimized TabNet:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      1021\n",
      "           1       0.78      0.83      0.81      1049\n",
      "\n",
      "    accuracy                           0.80      2070\n",
      "   macro avg       0.80      0.80      0.80      2070\n",
      "weighted avg       0.80      0.80      0.80      2070\n",
      "\n",
      "Confusion Matrix for Optimized TabNet:\n",
      " [[782 239]\n",
      " [180 869]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['optimized_tabnet_model.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the TabNet model with the best hyperparameters\n",
    "## this is training the model by the best hyperparameters\n",
    "optimized_tabnet_clf = TabNetClassifier(\n",
    "    n_d=best_params['n_d'],\n",
    "    n_a=best_params['n_a'],\n",
    "    n_steps=best_params['n_steps'],\n",
    "    gamma=best_params['gamma'],\n",
    "    lambda_sparse=best_params['lambda_sparse'],\n",
    "    optimizer_params={'lr': best_params['lr']},\n",
    "    mask_type=best_params['mask_type'],\n",
    "    n_shared=best_params['n_shared'],\n",
    "    n_independent=best_params['n_independent']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Perform cross-validation to evaluate model performance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_cv_pred = cross_val_predict(optimized_tabnet_clf, features_resampled, target_resampled, cv=cv)\n",
    "\n",
    "# Calculate cross-validated accuracy\n",
    "cv_accuracy = accuracy_score(target_resampled, y_cv_pred)\n",
    "print(\"Cross-Validated Accuracy:\", cv_accuracy)\n",
    "\n",
    "# Calculate cross-validated precision, recall, and F1-score\n",
    "cv_precision = precision_score(target_resampled, y_cv_pred)\n",
    "cv_recall = recall_score(target_resampled, y_cv_pred)\n",
    "cv_f1 = f1_score(target_resampled, y_cv_pred)\n",
    "\n",
    "print(f\"Cross-Validated Precision: {cv_precision:.2f}\")\n",
    "print(f\"Cross-Validated Recall: {cv_recall:.2f}\")\n",
    "print(f\"Cross-Validated F1 Score: {cv_f1:.2f}\")\n",
    "\n",
    "# Train the optimized model on the full dataset\n",
    "optimized_tabnet_clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "\n",
    "# Make final predictions on the test set\n",
    "y_pred_optimized = optimized_tabnet_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "print(\"Classification Report for Optimized TabNet:\\n\", classification_report(y_test, y_pred_optimized))\n",
    "print(\"Confusion Matrix for Optimized TabNet:\\n\", confusion_matrix(y_test, y_pred_optimized))\n",
    "\n",
    "# Save the trained model in a deployable format\n",
    "joblib.dump(optimized_tabnet_clf, 'optimized_tabnet_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dea7f4f0-6200-434e-8008-cf434f08aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7043 non-null   float64\n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(2), int64(2), object(17)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('no_missing_values_customer_data.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64125c-13f5-471b-81d1-a0641e1cec70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
